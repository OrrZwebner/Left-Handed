{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Script:\n",
    "1. Create Dataset of unkown/right reviews - according to the product (by ID), length of the review, and rating \n",
    "2. Merge to a final dataset - include label, text (of the review), 1:10 ratio (left:right)\n",
    "3. Slice 1:1 ratio final dataset\n",
    "4. All files are saved on the local pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iBpYyDLzIVwu"
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "import pandas as pd\n",
    "# Array\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import sys,os,json\n",
    "from pathlib import Path\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import sys,os,json,csv\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import bisect \n",
    "import json\n",
    "\n",
    "# Threading package\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Left_Reviews:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        a class to pull the datasets and create right reviews \n",
    "        \n",
    "        Input\n",
    "        - name: The name of the Dataset\n",
    "        - path : Pass to the json file\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.path = os.getcwd()\n",
    "        self.df_lefties = pd.DataFrame(data=None) #  dataframe of left users\n",
    "        self.df_left_IDs_reviews = pd.DataFrame(data=None) # reviews that were written by left IDs without the left phrases reviews (contains a left phrase such as 'im left handed')\n",
    "        self.df_total_reviews = pd.DataFrame(data=None) # total reviews without the text\n",
    "\n",
    "        \n",
    "    def execute(self, str_lower = True, total_reviews = False, create_right_dataset = False, export_left_ID_reviews = True):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        :param str_lower (boolean) - control wether to lower all the capital letter from the left ID reviews, remove puncutation\n",
    "        :param total_reviews (boolean) - control wether to create a dataframe of all the reviews (without text)\n",
    "        :param create_right_dataset - create a dateset of non - left handed reviews (assuming it was written by a right handed)\n",
    "        :param export_left_ID_reviews - export the left ID reviews to the original csv file (from sript 1) and add a column of matching reviews\n",
    "        Run all the relevant functions on the current reviews dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        if not total_reviews and create_right_dataset: # check that the input is logical - create right data_set only with total_reviews\n",
    "            print('Unable to create right dataset without pulling total reviews')\n",
    "            return\n",
    "        \n",
    "        \n",
    "        threads = [] # create a list of Concurrent Fututre objects\n",
    "        start = time.time()\n",
    "        self.pull_csvs(total_reviews = total_reviews)\n",
    "        with pd.read_csv(self.path+'/csv_files/'+self.name+'.csv', chunksize=100000) as reader: # iterate over the csv file in chunks\n",
    "            for chunk in reader:\n",
    "                with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                    thrd = executor.submit(self.slice_left_IDs_reviews, chunk_reviews = chunk, str_lower = str_lower) # assign execute function to thrd \n",
    "                    threads.append(thrd) \n",
    "        \n",
    "        counter = -1\n",
    "        for thrd in threads: # print if thread is done, backup solution were not asked.\n",
    "            if thrd.done():\n",
    "                counter += 1\n",
    "                print(f'chunk {counter} of dataset {self.name} is completed') # \n",
    "        \n",
    "        if create_right_dataset:\n",
    "            self.create_right_reviews_data()\n",
    "            \n",
    "        \n",
    "        if export_left_ID_reviews: # export the results to updated csv file (indluding the right reviews indeces)\n",
    "            self.export_results()\n",
    "\n",
    "\n",
    "    def pull_csvs(self, total_reviews = False):\n",
    "        \"\"\"\n",
    "        create dataframes of the relevant csv's\n",
    "        Input:\n",
    "        :param total_reviews (boolean) - control wether to create a dataframe of all the reviews (without text)\n",
    "        \"\"\"\n",
    "        self.df_lefties = pd.read_csv(self.path+'/results/'+self.name+'/left_ID\\'s.csv', index_col = 0) # read left ID's csv\n",
    "        self.df_left_IDs_reviews = pd.read_csv(self.path+'/results/'+self.name+'/left_ID_reviews.csv', index_col = 0) # read left ID's reviews csv\n",
    "        self.df_left_IDs_reviews['reviewText'] = np.nan # add a column of nans as text in order to replace it with the original text\n",
    "        if total_reviews: # only if chosen - add a datframe of the total reviews\n",
    "            self.df_total_reviews = pd.read_csv(self.path+'/results/'+self.name+'/total_reviews.csv', index_col = 0)\n",
    "    \n",
    "    \n",
    "    def slice_left_IDs_reviews(self, chunk_reviews, str_lower = True):\n",
    "        \"\"\"\n",
    "        find left IDs reviews in the current chunk (dataframe)\n",
    "        save them as a dataframe including the text (the diffrence from the exist csv file from srcipt 1). slice the reviews according to the index review\n",
    "        Input:\n",
    "        :param chunk (df) - chunk of original reviews\n",
    "        :param str_lower (boolean) - lowercase the text of the review and remove puncuation\n",
    "        \"\"\"\n",
    "        chunk_left_IDs_reviews =  chunk_reviews[chunk_reviews.index.isin(self.df_left_IDs_reviews.index)]  # create a sliced dataframe of lefties from the chunk according to line index\n",
    "        # double check according to left IDs\n",
    "        if not chunk_left_IDs_reviews['reviewerID'].isin(self.df_lefties['reviewerID']).all():\n",
    "            disply(chunk_left_IDs_reviews)\n",
    "            print(f'oops. there is a problem with the next reviews:\\n{display(chunk_left_IDs_reviews[chunk_left_IDs_reviews[\"reviewerID\"].isin(self.df_lefties[\"reviewerID\"]) == False])}')\n",
    "        # append the reviews to the left IDs reviews using pd.combine_first function. reorder the columns afterwards                                        \n",
    "        self.df_left_IDs_reviews = self.df_left_IDs_reviews.combine_first(other=chunk_left_IDs_reviews).loc[:,['reviewerID', 'asin', 'overall', 'n_words', 'LeftReview', 'reviewText']]\n",
    "        if str_lower:\n",
    "            self.df_left_IDs_reviews['reviewText'] = self.df_left_IDs_reviews['reviewText'].str.lower().str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    \n",
    "    def generate_right_reviews(self, row = None, n_match_rows=10, len_percentage=0.1):\n",
    "        \"\"\"\n",
    "        generate data of right reviews similar to left ID review according to 3 parameters:\n",
    "        asin (str) - unique ID of a product\n",
    "        overall (int) - rating of the review (by the ID). will be convereted to low, med, high (l, m ,h)\n",
    "        length (int) - length of the review, will be converted to interval by precnetage\n",
    "        Input:\n",
    "        :param row (1-D np.array) - a row of a left ID review\n",
    "        :param n_match_rows - number of matching rows to return. default 10\n",
    "        :param len_percentage - the interval (two sided full) percentage of the review length\n",
    "        Returns serie of review indeces that match the parameters\n",
    "        \"\"\"\n",
    "#         print(f'row: {row}\\n n:{n_match_rows}\\nlen:{len_percentage}')\n",
    "        if not isinstance(row, pd.Series): # check if the input is not wrong\n",
    "            print(f'Wrong row input /ntype: {type(row)}')\n",
    "            return\n",
    "        \n",
    "        convert_overal = lambda rate_num: 'h' if rate_num>3 else('l' if rate_num<3 else 'm') # lambda function to convert rate into low, medium or high (after building it has been fixed in the first script)\n",
    "        convert_len_to_interval = lambda length: [length*(1 - len_percentage),length*(1 + len_percentage)] # lambda function to convert the length to a list the represnt interval (min and max value)\n",
    "        \n",
    "        # create current row paramteres\n",
    "        cur_overal = row['overall'] # pandas\n",
    "        cur_length = row['n_words'] # pandas\n",
    "        cur_len_interval = convert_len_to_interval(cur_length)\n",
    "        \n",
    "        # add column of generated right reviews to avoid duplicated learned right reviews  - \n",
    "        #### TBD ####\n",
    "        right_genreated_reviews = np.zeros(shape=self.df_total_reviews.shape[0], dtype=int) # create a column of zeros for marking the generated right reviews\n",
    "        \n",
    "        # add the marking column (of marked right reviews) to total reviews\n",
    "        self.df_total_reviews['right_genreated_reviews'] = right_genreated_reviews.astype('int32') \n",
    "        \n",
    "        # after generate the data again - use the follow slicing\n",
    "        match_data = self.df_total_reviews[(self.df_total_reviews['asin'] == row ['asin'])  # pandas  slice by Product ID\n",
    "                                           & (self.df_total_reviews['Left_ID_Review'] == 0) # non left ID review\n",
    "                                           & (self.df_total_reviews['overall'] == cur_overal) # same overall (l/m/h)\n",
    "                                           & (self.df_total_reviews['right_genreated_reviews'] == 0) # only ungenerated reviews (right reviews that weren't picked for other left reviews)\n",
    "                                          ]\n",
    "        \n",
    "        best_matched_data = match_data.iloc[(match_data['n_words'] - cur_length).abs().argsort()][:n_match_rows] # slice the best n matching rows by number of rows \n",
    "        self.df_total_reviews[self.df_total_reviews.index.isin(best_matched_data.index)]['right_genreated_reviews'] = 1 # mark the genereated right reviews\n",
    "        \n",
    "\n",
    "        if best_matched_data.empty: # if the matching dataframe is emtpy  - return None instead of empty set\n",
    "            return None\n",
    "           \n",
    "         # else - return a set of the indeces   \n",
    "        return set(best_matched_data.index) \n",
    "\n",
    "        \n",
    "        \n",
    "    def create_right_reviews_data(self, n_match_rows=10, len_percentage=0.1):\n",
    "        \"\"\"\n",
    "        create data of right reviews similar to all the left ID reviews according to 3 parameters:\n",
    "        asin (str) - unique ID of a product\n",
    "        overall (int) - rating of the review (by the ID). will be convereted to low, med, high (l, m ,h)\n",
    "        length (int) - length of the review, will be converted to interval by precnetage (currently unused)\n",
    "        Input:\n",
    "        :param n_match_rows - number of matching rows to return. default 10\n",
    "        :param len_percentage - the interval (two sided full) percentage of the review length\n",
    "        add a feature 'RightReviews' to self.df_left_IDs_reviews\n",
    "        \"\"\"\n",
    "        start = time.time()                                   \n",
    "        df_match_right_reviews = self.df_left_IDs_reviews.apply(func=self.generate_right_reviews,axis=1, args=(n_match_rows, len_percentage))                               \n",
    "        self.df_left_IDs_reviews['Right_reviews_Dataset'] = df_match_right_reviews\n",
    "        print(f'{self.name} create right reviews data: {time.time() - start} seconds')   \n",
    "\n",
    "        \n",
    "    \n",
    "    def disribute_left_reviews_length(self, upper_threshold=20):\n",
    "        \"\"\"\n",
    "        create distribution of the number of words of short reviews.\n",
    "        Input:\n",
    "        :param upper_threshold (int) - the maximum number of words reviews to include in the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6,4), dpi = 120)\n",
    "        sns.histplot(data=self.df_left_IDs_reviews[self.df_left_IDs_reviews['n_words'] <= upper_threshold], bins=10, x ='n_words', kde=True);\n",
    "        plt.xlabel('number of words')\n",
    "        plt.ylabel('count')\n",
    "        plt.title(f'distribution of number of words per \\'left\\' review \\n{self.name}\\n', fontsize=12)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def check_left_IDs_reviews_duplications(self):\n",
    "        return self.df_left_IDs_reviews[self.df_left_IDs_reviews.duplicated(subset='reviewText', keep=False)]\n",
    "        \n",
    "        \n",
    "    def export_results(self, threshold=0):\n",
    "        \"\"\"\n",
    "        export the results to csv files.\n",
    "        Input:\n",
    "        :param threshold (int) - minimum number of words for review\n",
    "        \"\"\"\n",
    "        path = os.path.join(os.getcwd(), 'results/'+self.name) # create a path to the results folder\n",
    "        if not os.path.exists(path): # if the directory doesn't exist, create it\n",
    "            os.mkdir(path)\n",
    "\n",
    "        # csv of left ID reviews\n",
    "        df_left_IDs_reviews = self.df_left_IDs_reviews[self.df_left_IDs_reviews['n_words'] > threshold] \n",
    "        df_left_IDs_reviews.to_csv(path + '/left_ID_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Office_Products_5_left_reviews = Left_Reviews(name='Office_Products_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Office_Products_5_left_reviews.execute(str_lower=False, total_reviews=True, create_right_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right reviews data set of Electorincs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electronics_left_reviews = Left_Reviews(name='Electronics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Electronics_left_reviews.execute(str_lower=False, total_reviews=False, create_right_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right reviews data set of Office Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Office_Products_left_reviews = Left_Reviews(name='Office_Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Office_Products_left_reviews.execute(str_lower=False, total_reviews=True, create_right_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right reviews data set of Arts_Crafts_and_Sewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arts_Crafts_and_Sewing_left_reviews = Left_Reviews(name='Arts_Crafts_and_Sewing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Arts_Crafts_and_Sewing_left_reviews.execute(str_lower=False, total_reviews=True, create_right_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right reviews data set of Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Home_and_Kitchen_left_reviews = Left_Reviews(name='Home_and_Kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Home_and_Kitchen_left_reviews.execute(str_lower=False, total_reviews=True, create_right_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right reviews data set of Tools_and_Home_Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools_and_Home_Improvement_left_reviews = Left_Reviews(name='Tools_and_Home_Improvement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tools_and_Home_Improvement_left_reviews.execute(str_lower=False, total_reviews=True, create_right_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalDataSet:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        A class to represent a single reviews database from Amazon Review Data (2018) - https://nijianmo.github.io/amazon/index.html\n",
    "        \n",
    "        Input\n",
    "        - namet: The name of the Dataset\n",
    "        - path : Pass to the json file\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.path = os.getcwd()\n",
    "        self.df_left_IDs_reviews = pd.read_csv(self.path+'/results/'+self.name+'/left_ID_reviews.csv', index_col = 0) # read left ID's reviews csv\n",
    "        self.df_right_reviews = None\n",
    "        self.indeces_df = pd.DataFrame(data=None, columns=['match_index', 'right_index'])\n",
    "        self.final_dataset = None\n",
    "    \n",
    "    \n",
    "    def execute(self, path=None, cols=None):\n",
    "        \"\"\"\n",
    "        execute all the method of FinalDataSet class\n",
    "        \"\"\"\n",
    "        self.remove_full_duplications()  # Remove left reviews with the identical 3 fields: 'reviewerID','asin','reviewText'\n",
    "        right_dataset_sanity = self.merge_df_right_indeces()\n",
    "        \n",
    "        if not right_dataset_sanity: # there is no right reviews dataset - break the execute function\n",
    "            return\n",
    "        \n",
    "        self.draw_right_reviews_from_json()\n",
    "        self.create_final_dataset()\n",
    "        print(f'Final Dataset is Ready as csv file and dataframe (attribute)')\n",
    "\n",
    "\n",
    "    def remove_full_duplications(self):\n",
    "        \"\"\"\n",
    "        Remove left reviews with the identical 3 fields: 'reviewerID','asin','reviewText'\n",
    "        Returns the new dataset\n",
    "        \"\"\"\n",
    "        # check duplications under 3 columns (fusion) - reviewr ID, asin review text\n",
    "        print(f'Number of reviews before removing duplications (identical reviewer ID, product asin and text): {self.df_left_IDs_reviews.shape[0]}')\n",
    "        df_after_remove = self.df_left_IDs_reviews[~self.df_left_IDs_reviews.duplicated(subset=['reviewerID','asin','reviewText'], keep='first')]\n",
    "        self.df_left_IDs_reviews = df_after_remove\n",
    "        print(f'\\nNumber of reviews after removing duplications: {self.df_left_IDs_reviews.shape[0]}')\n",
    "        return self.df_left_IDs_reviews\n",
    "    \n",
    "    \n",
    "    def check_left_IDs_reviews_duplications(self):\n",
    "        \"\"\"\n",
    "        Return all duplicated rows (all the duplications)\n",
    "        \"\"\"\n",
    "        return self.df_left_IDs_reviews[self.df_left_IDs_reviews.duplicated(subset=['reviewerID','asin','reviewText'], keep=False)]\n",
    "    \n",
    "\n",
    "    \n",
    "    def merge_df_right_indeces(self):\n",
    "        \"\"\"\n",
    "        Returns a boolean arg if there are sets of right indeces in the Left_ID_reviews.\n",
    "        Edit the the attribute indeces_df [right review index, match index (of left ID review)]\n",
    "        as a merged dataframe of all uniqe right reviews indeces according to each left ID review set\n",
    "        \"\"\"\n",
    "        indeces_df = pd.DataFrame(data=None, columns=['match_index', 'right_index']) # an empty list for future merged list\n",
    "\n",
    "        # check if there is right reviews data set in the left ID reviews\n",
    "        if not 'Right_reviews_Dataset' in self.df_left_IDs_reviews.columns:\n",
    "            print('There is no right reviews dataset. Check the Left Rreview instance.')\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        for left_index, left_review in self.df_left_IDs_reviews.iterrows(): # iterate over all the rows in the dataframe (index, all the other cols)\n",
    "            indeces_set = left_review['Right_reviews_Dataset']\n",
    "            if indeces_set == 'None' or indeces_set == 'set()' or not indeces_set or pd.isna(indeces_set):  \n",
    "                continue\n",
    "\n",
    "            indeces_set = indeces_set.strip('{}') # remove brackets\n",
    "            temp_indeces_list = list(map(int, indeces_set.split(', ')))\n",
    "            temp_df = pd.DataFrame(data=None, columns=['match_index', 'right_index']) # creat a temporary 2 column df of the left reviews index and all the right reviews            \n",
    "            temp_df['right_index'] = temp_indeces_list\n",
    "            temp_df['match_index'] = left_index # a single value - the index of the left review\n",
    "            indeces_df = indeces_df.append(temp_df)\n",
    "            \n",
    "        indeces_df = indeces_df.sort_values(by='right_index', axis=0) # sort the dataframe according to the right indeces\n",
    "        indeces_df = indeces_df.drop_duplicates(subset='right_index', keep='first')\n",
    "\n",
    "        self.indeces_df = indeces_df\n",
    "        return True\n",
    "\n",
    "    \n",
    "        \n",
    "    def draw_right_reviews_from_json(self, path='json_files/', cols=['reviewerID','asin','overall','reviewText']):\n",
    "        \"\"\"\n",
    "        Input:        \n",
    "        - self.indeces_df: (df) of right reviews indeces\n",
    "        - path: (string) to the directory of the json file (directory only - without name of the file)\n",
    "        - cols: (list of strings) names of the cols to save as dataframe\n",
    "        - self.name: (string) of the dataset\n",
    "        Creates a csv of the generated right reviews out of the original json files from amazon\n",
    "        \"\"\"\n",
    "        print(f'\\nstart draw_right_reviews of {self.name}:')\n",
    "        start_time = time.time() # calculate time of running\n",
    "        file = Path(path + self.name + '.json') # create a path to the json file\n",
    "        csv_file = open('csv_files/right_reviews/'+self.name+'.csv', 'w') # create and open a csv file named by the name of the dataset in the csv directory\n",
    "        csv_writer = csv.writer(csv_file) # create a csv write object\n",
    "        csv_writer.writerow(['left_matching','right_index'] + cols) # create columns header to the csv file, including original review index column\n",
    "        cur_index = 0 # index of the current line\n",
    "        cur_pointer = 0 # pointer in the list of right reviews indeces\n",
    "        right_line_index = self.indeces_df.iloc[cur_pointer, 1] # index of the current right review row\n",
    "        \n",
    "        with file.open('r') as f:\n",
    "            for line in f:\n",
    "                if cur_pointer == self.indeces_df.shape[0]: # if we finished iterating over total the right reviews - break the loop\n",
    "                    print(f'cur pointer: {cur_pointer} \\nlength of indeces_list:{self.indeces_df.shape[0]}')\n",
    "                    break\n",
    "\n",
    "                if cur_index == self.indeces_df.iloc[cur_pointer, 1]:\n",
    "                    # if the current index is right review - append it to the dataframe\n",
    "                    line_dict = json.loads(line) # load the row to a dictionary\n",
    "                    line_list = [self.indeces_df.iloc[cur_pointer, 0], cur_index] # list of the row values. start with the index of the right reviews and the current index as the index of the row for the future csv\n",
    "\n",
    "                    for key in cols:\n",
    "                        if key not in line_dict: # add field only of it is exists\n",
    "                            line_dict[key] = \"\"\n",
    "                        line_list.append(line_dict[key]) # add the current key to the line list        \n",
    "                    csv_writer.writerow(line_list) # write the rows to the csv file\n",
    "                    cur_pointer += 1  # pointer of the next right reviews index\n",
    "\n",
    "                cur_index += 1  # index of the next row\n",
    "        # author note - for some reason the right_reviews csv doesn't include matching index (of the left reviews - to be fixed in the next version!!)\n",
    "        print(f\"done converting \\\"{self.name}\\\" right reviews to csv file, {cur_index} lines were checked, {cur_pointer} reviews were drawn \\nrun time: {time.time()-start_time:.3f} seconds\")         \n",
    "        self.df_right_reviews = pd.read_csv(filepath_or_buffer='csv_files/right_reviews/'+self.name+'.csv',header=0, index_col=1)\n",
    "\n",
    "        \n",
    "        \n",
    "    def create_final_dataset(self):\n",
    "        \"\"\"\n",
    "        Returns a final dataset of reviews and labels (left - 1, right - 0)\n",
    "        export the dataset to a csv file \n",
    "        \"\"\"\n",
    "        df_final_right_reviews = self.df_right_reviews.reset_index()  # temp df for the right reviews\n",
    "        df_final_right_reviews['label'] = np.zeros(shape=df_final_right_reviews.shape[0], dtype=int)  # add a label of zeroes (as right reviews)\n",
    "\n",
    "        left_reviews_tostack = pd.DataFrame(data={'index': self.df_left_IDs_reviews.index, 'left_matching': self.df_left_IDs_reviews.index, 'reviewText': self.df_left_IDs_reviews['reviewText'], \\\n",
    "                                                  'label': np.ones(shape=self.df_left_IDs_reviews.shape[0], dtype=int)}) # create a df of left reviews for stacking\n",
    "\n",
    "        self.final_dataset = pd.DataFrame(data = np.vstack((df_final_right_reviews[['right_index', 'left_matching','reviewText', 'label']], left_reviews_tostack)),\\\n",
    "                                          columns=['index','left_matching','reviewText', 'label']).set_index(keys=['index'])\n",
    "        self.final_dataset['category'] = self.name\n",
    "        self.final_dataset = self.final_dataset[['category','left_matching','reviewText', 'label']]\n",
    "        self.final_dataset.to_csv(path_or_buf=self.path+'/results/'+self.name+'/final_labeled_dataset.csv')\n",
    "        return self.final_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Office_Products_5_training_data = FinalDataSet(name = 'Office_Products_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Office_Products_5_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data - Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "electronics_training_data = FinalDataSet(name = 'Electronics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "electronics_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data - Office_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Office_Products_training_data = FinalDataSet(name = 'Office_Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Office_Products_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data - Arts_Crafts_and_Sewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Arts_Crafts_and_Sewing_training_data = FinalDataSet(name = 'Arts_Crafts_and_Sewing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Arts_Crafts_and_Sewing_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data - Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Home_and_Kitchen_training_data = FinalDataSet(name = 'Home_and_Kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Home_and_Kitchen_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data - Tools_and_Home_Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tools_and_Home_Improvement_training_data = FinalDataSet(name = 'Tools_and_Home_Improvement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tools_and_Home_Improvement_training_data.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated joint csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = ['Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "final_combined_labeled_dataset = None\n",
    "for dataset in datasets:  \n",
    "    final_combined_labeled_dataset = pd.concat([final_labeled_dataset, pd.read_csv('results/' + dataset + '/final_labeled_dataset.csv', index_col=0)])\n",
    "final_combined_labeled_dataset.to_csv(path_or_buf = 'results/final_combined_labeled_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Electronics_duplications = Electronics_left_reviews.check_left_IDs_reviews_duplications()\n",
    "Electronics_duplications = Electronics_duplications[Electronics_duplications['n_words']>10] # slice only review with more than 10 words\n",
    "# display duplicaterd reviews with diffrent reviewer ID\n",
    "print(f'Duplicated reviews with diffrent IDs: \\n')\n",
    "display(Electronics_duplications[(Electronics_duplications['reviewerID'] != Electronics_duplications['reviewerID']) \n",
    "        & (Electronics_duplications['reviewText'] == Electronics_duplications['reviewText'])])\n",
    "# display full duplications\n",
    "print(f'Full duplications: \\n')\n",
    "display(Electronics_duplications[Electronics_duplications.duplicated(subset=['reviewerID','asin','reviewText'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for review_object in Left_Reviews_list:\n",
    "    print(f'Dataset {review_object.name}, total Left IDs reviews: {review_object.df_left_IDs_reviews.shape[0]}')\n",
    "    display(review_object.check_left_IDs_reviews_duplications())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words distirubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for review_object in Left_Reviews_list:\n",
    "    review_object.disribute_left_reviews_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reviews_n_words(threshold=10):\n",
    "    \"\"\"\n",
    "    Count the number of reviews with more than a given threshold and less\n",
    "    :param threshold (int) - minimum number of words for review\n",
    "    print for each dataset\n",
    "    \"\"\"\n",
    "    for review_object in Left_Reviews_list:\n",
    "        df_left_ID_reviews = review_object.df_left_IDs_reviews[review_object.df_left_IDs_reviews['LeftReview'] == 0] # remove the original left reviews \n",
    "        df_threshold = df_left_ID_reviews[df_left_ID_reviews['n_words'] > threshold] # check reviews with more than 10 words\n",
    "        print(f'{review_object.name}:\\nnumber of left reviews: {df_left_ID_reviews.shape[0]} \\nnumber of left reviews with more than {threshold} words: {df_threshold.shape[0]} \\nnumber of left reviews with {threshold} or less words: {df_left_ID_reviews.shape[0] - df_threshold.shape[0]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reviews_n_words(threshold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review_object in Reviews_list:\n",
    "    review_object.export_results(threshold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets for setfit - only label and texts features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final labaled datsets of all categories without unnecesarry features\n",
    "datasets = ['Office_Products_5']\n",
    "# datasets = ['Office_Products_5','Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "for dataset in datasets:\n",
    "    temp_df = pd.read_csv(os.getcwd()+'/results/' + dataset + '/final_labeled_dataset.csv', index_col=0) # load the original dataset\n",
    "    temp_df.to_csv(path_or_buf=os.getcwd()+'/results/' + dataset + '/final_labeled_dataset_setfit.csv', columns=['reviewText','label']) # push it sliced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice datasets of 1:1 (left:right) out of 1:10 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Left review match the Right review that has the most similar length (number of words)\n",
    "31.5.23 - for some reason there was a problem in right_reviews.csv. I had to fix it here manuallu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratio_dataset(datasets=[None], ratio=1, name=None, directory=os.getcwd()):\n",
    "    \"\"\"\n",
    "    Input:        \n",
    "    - datasets: (list of strings) names of the datasets to create new datasets for (name of amazon datasets)\n",
    "    - ratio: (int) how many right reviews to slice, default 1. Should be less or equal the the original n_match_rows in the Left_reviews class.\n",
    "    - name: (str) name of the new csv file. \n",
    "    - directory: (str) - directory of the project \n",
    "    Saves a new final dataset  - change the number of matching right reviews per left review according to the ratio\n",
    "    Add features to the right reviews csv file - left_matching (According to the final_labeled_dataset), rank - measure how 'similar' the right review to the matching left review\n",
    "    *Fix the right_reivews csv files\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    if not name: # if there is no input for the name - insert the default\n",
    "        name='final_dataset_1:'+str(ratio)+'_ratio' # Default name is according to the ratio\n",
    "        \n",
    "    for dataset in datasets:\n",
    "        if not os.path.exists(directory+'/results/' + dataset): # check that path is legal\n",
    "            print(f'There is no dataset {dataset} in the directory')\n",
    "            continue\n",
    "\n",
    "        total_reviews = pd.read_csv(directory+'/results/' + dataset + '/total_reviews.csv', index_col=0) # load the right reviews dataset (including number of words)\n",
    "        final_labaled_dataset = pd.read_csv(directory+'/results/' + dataset + '/final_labeled_dataset.csv', index_col=0) # load the original dataset (including the matchings. in the future version - include matchings in the right reviews dataset)\n",
    "        # fix the right+left reviews dataset. Push the data from final_labaled_dataset and total_reviews\n",
    "        left_reviews = final_labaled_dataset[final_labaled_dataset['label']==1].join(other=total_reviews[['reviewerID','asin','overall','n_words']], how='left')[['category','left_matching','reviewerID','asin','overall','n_words','reviewText']]\n",
    "        right_reviews = final_labaled_dataset[final_labaled_dataset['label']==0].join(other=total_reviews[['reviewerID','asin','overall','n_words']], how='left')[['category','left_matching','reviewerID','asin','overall','n_words','reviewText']] \n",
    "\n",
    "        # compute the ranks of the review according to the number of words (closet to the left review)\n",
    "        right_reviews['left_n_words'] = right_reviews.apply(func=lambda row: left_reviews['n_words'].loc[row['left_matching']], axis=1, raw=False) # add a column the number of words of the matching review\n",
    "        right_reviews['diffrence'] = abs(right_reviews['left_n_words'] - right_reviews['n_words']) # comptue the diffrence between the review and the left review (number of words)\n",
    "        \n",
    "        right_reviews['rank'] = right_reviews.groupby('left_matching')['diffrence'].rank (method='first') # rank according to the diffrence (minimum)\n",
    "        count_ones = sum(right_reviews['rank'] == 1.0)\n",
    "\n",
    "        print(f\"\\nThe number of times 1.0 appears in {dataset} 'rank': {count_ones}\")\n",
    "        # map the ranks to the final labaled dataset\n",
    "        final_labaled_dataset['rank'] = final_labaled_dataset.index \n",
    "        final_labaled_dataset['rank'] = final_labaled_dataset['rank'].map(right_reviews['rank'])\n",
    "        final_labaled_dataset.loc[final_labaled_dataset['label'] == 1, 'rank'] = 0 # the left reviews rank as 0 - for the method of slicings the ratio later\n",
    "        final_new_ratio_dataset = final_labaled_dataset[final_labaled_dataset['rank'] <= ratio]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # run some test n the datframes\n",
    "\n",
    "        # Get unique index values from left_reviews\n",
    "        left_indices = left_reviews.index.unique()\n",
    "\n",
    "        # Check which index values are present in the 'left_matching' column of right_reviews\n",
    "        matching_indices = right_reviews['left_matching'].isin(left_indices)\n",
    "\n",
    "        # Count the unique matching indices in 'left_matching' column\n",
    "        unique_matching_count = right_reviews.loc[matching_indices, 'left_matching'].nunique()\n",
    "\n",
    "        # Check if each index in left_reviews has at least one matching row in right_reviews\n",
    "        if unique_matching_count == len(left_indices):\n",
    "            print(\"For each index in left_reviews, there is at least one row in right_reviews with a matching 'left_matching' value.\")\n",
    "        else:\n",
    "            print(f\"{len(left_indices) - unique_matching_count} indices from left_reviews don't have a matching row in right_reviews.\")\n",
    "        \n",
    "        \n",
    "        if final_new_ratio_dataset.index.is_unique:\n",
    "            print(\"All indices in final_new_ratio_dataset are unique.\")\n",
    "        else:\n",
    "            print(\"There are duplicate indices in final_new_ratio_dataset.\")\n",
    "  \n",
    "\n",
    "        # export the datasets to csv files\n",
    "        final_new_ratio_dataset.to_csv(path_or_buf=directory+'/results/' + dataset + '/' + name +'.csv') # push it sliced\n",
    "        left_reviews.to_csv(path_or_buf=directory + '/results/' + dataset + '/left_reviews.csv')\n",
    "        right_reviews.to_csv(path_or_buf=directory + '/results/' + dataset + '/right_reviews.csv')\n",
    "        \n",
    "        test_balance(df=final_new_ratio_dataset, dataset=dataset)\n",
    "        \n",
    "#         return final_new_ratio_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of times 1.0 appears in Office_Products_5 'rank': 1362\n",
      "17 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Office_Products_5\n",
      "\n",
      "Count of rows labeled 1: 1379\n",
      "Count of rows labeled 0: 1362\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n",
      "\n",
      "The number of times 1.0 appears in Office_Products 'rank': 2047\n",
      "41 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Office_Products\n",
      "\n",
      "Count of rows labeled 1: 2089\n",
      "Count of rows labeled 0: 2047\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n",
      "\n",
      "The number of times 1.0 appears in Arts_Crafts_and_Sewing 'rank': 710\n",
      "62 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Arts_Crafts_and_Sewing\n",
      "\n",
      "Count of rows labeled 1: 772\n",
      "Count of rows labeled 0: 710\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n",
      "\n",
      "The number of times 1.0 appears in Electronics 'rank': 2963\n",
      "67 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Electronics\n",
      "\n",
      "Count of rows labeled 1: 3030\n",
      "Count of rows labeled 0: 2963\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n",
      "\n",
      "The number of times 1.0 appears in Home_and_Kitchen 'rank': 4850\n",
      "153 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Home_and_Kitchen\n",
      "\n",
      "Count of rows labeled 1: 5003\n",
      "Count of rows labeled 0: 4850\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n",
      "\n",
      "The number of times 1.0 appears in Tools_and_Home_Improvement 'rank': 1033\n",
      "48 indices from left_reviews don't have a matching row in right_reviews.\n",
      "All indices in final_new_ratio_dataset are unique.\n",
      "dataset Tools_and_Home_Improvement\n",
      "\n",
      "Count of rows labeled 1: 1081\n",
      "Count of rows labeled 0: 1033\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n"
     ]
    }
   ],
   "source": [
    "# datasets = ['Office_Products_5']\n",
    "datasets = ['Office_Products_5','Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "create_ratio_dataset(datasets, name='balanced_labeled_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some test on te full balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Office_Products_5','Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "\n",
    "for dataset in datasets: # iterate over the datasets\n",
    "    \n",
    "    df = pd.read_csv(os.getcwd()+'/results/'+dataset+'/balanced_labeled_dataset.csv', index_col=0) # load the balanced dataset \n",
    "#     test_balance(df=df, dataset=dataset)\n",
    "    \n",
    "    \n",
    "def test_balance(df=None, dataset=''):\n",
    "    \"\"\"\n",
    "    test duplications and number of ranks\n",
    "    \"\"\"\n",
    "    print(f'dataset {dataset}\\n')\n",
    "    # 1. Check how many rows are labeled 1 and how many are labeled 0\n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(f\"Count of rows labeled 1: {label_counts.get(1, 0)}\")\n",
    "    print(f\"Count of rows labeled 0: {label_counts.get(0, 0)}\")\n",
    "\n",
    "    # 2. Ensure no duplicate indices\n",
    "    duplicate_indices = df.index.duplicated(keep=False)\n",
    "    if any(duplicate_indices):\n",
    "        print(\"There are duplicate indices.\")\n",
    "        # Display the rows corresponding to the duplicated indices\n",
    "        duplicated_rows = df.loc[df.index[duplicate_indices]]\n",
    "        print(\"Rows with duplicated indices:\")\n",
    "        print(duplicated_rows)\n",
    "    else:\n",
    "        print(\"There are no duplicate indices.\")\n",
    "\n",
    "    # 3. Check if all cells in 'reviewText' are strings\n",
    "    all_strings = all(df['reviewText'].apply(lambda x: isinstance(x, str)))\n",
    "    if all_strings:\n",
    "        print(\"All cells in 'reviewText' are strings.\")\n",
    "    else:\n",
    "        print(\"Not all cells in 'reviewText' are strings.\")\n",
    "        # Optionally, print the indices and values of non-string cells\n",
    "        non_string_cells = df[df['reviewText'].apply(lambda x: not isinstance(x, str))]\n",
    "        print(\"Indices and values of non-string cells in 'reviewText':\")\n",
    "        print(non_string_cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset Office_Products_5\n",
      "\n",
      "Count of rows labeled 1: 1379\n",
      "Count of rows labeled 0: 1362\n",
      "There are no duplicate indices.\n",
      "All cells in 'reviewText' are strings.\n"
     ]
    }
   ],
   "source": [
    "test_balance(df_test, dataset='Office_Products_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create full balanced labaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name='Office_Products'\n",
    "O_5 = pd.read_csv(os.getcwd()+'/results/'+name+'/balanced_labeled_dataset.csv', index_col=0) # load the right reviews dataset (including number of words)\n",
    "O_5       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_index</th>\n",
       "      <th>category</th>\n",
       "      <th>left_matching</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7479</td>\n",
       "      <td>Office_Products</td>\n",
       "      <td>4373629</td>\n",
       "      <td>I bought these to include thank you cards in the packages I mail out for my store and they are the most absolute cards I've ever seen.  Even the envelopes are super cute; they have little cupcakes tied with tags that say \"Merci\" on the front. They seem slightly smaller than regular notecards but that sort of adds to their whimsy.  I liked them so much I bought two more boxes and will probably buy more when I run out.  :)</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10670</td>\n",
       "      <td>Office_Products</td>\n",
       "      <td>10631</td>\n",
       "      <td>DOWNTON ABBEY has won the hearts of countless millions, including myself. I was glad to see this calendar offered on AMAZON and at a good price. This is a great calendar that is well-made and well-designed. It is on heavy paper stock with glossy pictures. This is not a cheap looking calendar on lightweight paper. It will last through the year and I plan to keep it as a memento of this great show.\\nI am disappointed with some of the pictures chosen for this calendar. I would assume they had countless pictures to choose some but picked several that are poorly framed or show the backs of servants or side views of faces looking downward. Why show characters not facing the camera or partially off to the side.  Yes, all the pictures are nice and show specific moments from the show, but it seems like the creators could have used better pictures. Let's see the characters full faces. . Interesting enough, the back of the calendar as seen in the listing has one deviation from the calendar I have. Mine does not have the picture of Mr. Bates and Anna holding teacups. Mine has Carson and Mrs. Hughes instead and that picture is a disappointment. It's a profile of them but they are each off to the side so the middle of the picture is just wide open space. I would have liked to see more faces and more close-ups but still is a nice calendar. It's a high-quality calendar that could have made use of better pictures.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11998</td>\n",
       "      <td>Office_Products</td>\n",
       "      <td>12011</td>\n",
       "      <td>One of the very best all around ferret books available. Detailed, up-to-date information on housing, feeding, ferret-specific ailments and training. This book gets into the nuts and bolts of day-to-day living with and caring for ferrets, its a must have book for both novice and experienced ferret owners. If you can only afford one ferret book, this is the one to get!</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14974</td>\n",
       "      <td>Office_Products</td>\n",
       "      <td>14897</td>\n",
       "      <td>good quality</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18364</td>\n",
       "      <td>Office_Products</td>\n",
       "      <td>18367</td>\n",
       "      <td>I wanted to love these! While I do like them, they didn't quite meet my expectations on all counts.\\n\\nThough my first grader son isn't homeschooled, he's quite ahead of his grade. We do a lot of educating at home to supplement the classroom so he stays challenged. I was particularly interested in these books because I grew up in Kentucky and missed a lot of the California state history. I thought these would be great for us to learn together.\\n\\nI think they WILL be great... eventually. The description seems to imply that these are good for a variety of ages, but they are just not geared toward a kid of his age. I would say an advanced third grader. It's actually less about the difficult of the prose, and more about the violence and dry nature of the material. It was a bit much for my kid. He alternated between bored and horrified when we read it together. I finished them on my own and found them very informative for my own knowledge.\\n\\nI also think these are quite worth the price. I expected them to be bigger or hardcover for that cost. However, they are slim paperback booklets. Overall, interesting and accurate, but not quite what they could be.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23573</th>\n",
       "      <td>8960818</td>\n",
       "      <td>Tools_and_Home_Improvement</td>\n",
       "      <td>8960818</td>\n",
       "      <td>Needed some light to maneuver around in the AM ,Someone tends to get crabby when I need a little light on to get ready for work and I turn lights on to move around the house,  So I am very satisfied with this product</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23574</th>\n",
       "      <td>8965606</td>\n",
       "      <td>Tools_and_Home_Improvement</td>\n",
       "      <td>8965606</td>\n",
       "      <td>I should start by saying that I received this product for free from Toogou for testing and review.\\n\\nThis flashlight is made pretty well, it is all milled aluminum.  There is an O-ring on the end cap to help with water resistance.  It has 5 modes  high, medium, low, strobe and SOS.  All work well and the brightness levels are distinct to give you a nice range.  The button clicks nicely to toggle through all the modes but there is no mode memory so you have to toggle through every mode every time you use it, not uncommon for a flashlight of this price range.  There is no Lumen listing but I would guess the high level is about 70 lumens.  The beam spread is decent and even with no hot spot.  When zoomed out it comes to a small bright square spot.  The light feels good in hand.\\n\\nThis comes with 1 18650 rechargeable battery with tube adaptor as well as an insert so you can use 3 AAA batteries.  I tried it with both and didn't notice any difference in performance or brightness.  I would guess the difference would be battery life.  A charger for the 18650 battery is also included as well as a cheap lanyard.\\n\\nThe only real issue with this is that the zoom feature is a bit loose when extended.  It doesnt stay out well and wiggles a bit.  Other than that this is a good flashlight for the price.  It doesnt compare to higher end flashlights obviously but it is solid and the fact that they include the 18650 battery makes this worth the price.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23575</th>\n",
       "      <td>8971765</td>\n",
       "      <td>Tools_and_Home_Improvement</td>\n",
       "      <td>8971765</td>\n",
       "      <td>Nice well made wire wheel but not sized properly to fit a 4 1/2 grinder. I installed on my larger grinder and reordered  from another manufacturer.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23576</th>\n",
       "      <td>8976956</td>\n",
       "      <td>Tools_and_Home_Improvement</td>\n",
       "      <td>8976956</td>\n",
       "      <td>I do a fair amount of residential light switch upgrades and an occasional ceiling fan install. This is a nice tool to have in the tool kit and does make pulling the wires out it the box easier, stuffing the wires back into to the box is much more precise using this tool.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23577</th>\n",
       "      <td>9006904</td>\n",
       "      <td>Tools_and_Home_Improvement</td>\n",
       "      <td>9006904</td>\n",
       "      <td>Need some solder to repair some small diameter wires, the solder worked perfectly with no issues.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23578 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category_index                    category  left_matching  \\\n",
       "0      7479            Office_Products             4373629         \n",
       "1      10670           Office_Products             10631           \n",
       "2      11998           Office_Products             12011           \n",
       "3      14974           Office_Products             14897           \n",
       "4      18364           Office_Products             18367           \n",
       "...      ...                       ...               ...           \n",
       "23573  8960818         Tools_and_Home_Improvement  8960818         \n",
       "23574  8965606         Tools_and_Home_Improvement  8965606         \n",
       "23575  8971765         Tools_and_Home_Improvement  8971765         \n",
       "23576  8976956         Tools_and_Home_Improvement  8976956         \n",
       "23577  9006904         Tools_and_Home_Improvement  9006904         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reviewText  \\\n",
       "0      I bought these to include thank you cards in the packages I mail out for my store and they are the most absolute cards I've ever seen.  Even the envelopes are super cute; they have little cupcakes tied with tags that say \"Merci\" on the front. They seem slightly smaller than regular notecards but that sort of adds to their whimsy.  I liked them so much I bought two more boxes and will probably buy more when I run out.  :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1      DOWNTON ABBEY has won the hearts of countless millions, including myself. I was glad to see this calendar offered on AMAZON and at a good price. This is a great calendar that is well-made and well-designed. It is on heavy paper stock with glossy pictures. This is not a cheap looking calendar on lightweight paper. It will last through the year and I plan to keep it as a memento of this great show.\\nI am disappointed with some of the pictures chosen for this calendar. I would assume they had countless pictures to choose some but picked several that are poorly framed or show the backs of servants or side views of faces looking downward. Why show characters not facing the camera or partially off to the side.  Yes, all the pictures are nice and show specific moments from the show, but it seems like the creators could have used better pictures. Let's see the characters full faces. . Interesting enough, the back of the calendar as seen in the listing has one deviation from the calendar I have. Mine does not have the picture of Mr. Bates and Anna holding teacups. Mine has Carson and Mrs. Hughes instead and that picture is a disappointment. It's a profile of them but they are each off to the side so the middle of the picture is just wide open space. I would have liked to see more faces and more close-ups but still is a nice calendar. It's a high-quality calendar that could have made use of better pictures.                                          \n",
       "2      One of the very best all around ferret books available. Detailed, up-to-date information on housing, feeding, ferret-specific ailments and training. This book gets into the nuts and bolts of day-to-day living with and caring for ferrets, its a must have book for both novice and experienced ferret owners. If you can only afford one ferret book, this is the one to get!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3      good quality                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "4      I wanted to love these! While I do like them, they didn't quite meet my expectations on all counts.\\n\\nThough my first grader son isn't homeschooled, he's quite ahead of his grade. We do a lot of educating at home to supplement the classroom so he stays challenged. I was particularly interested in these books because I grew up in Kentucky and missed a lot of the California state history. I thought these would be great for us to learn together.\\n\\nI think they WILL be great... eventually. The description seems to imply that these are good for a variety of ages, but they are just not geared toward a kid of his age. I would say an advanced third grader. It's actually less about the difficult of the prose, and more about the violence and dry nature of the material. It was a bit much for my kid. He alternated between bored and horrified when we read it together. I finished them on my own and found them very informative for my own knowledge.\\n\\nI also think these are quite worth the price. I expected them to be bigger or hardcover for that cost. However, they are slim paperback booklets. Overall, interesting and accurate, but not quite what they could be.                                                                                                                                                                                                                                                                                                       \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...                                                                                                                                                                                                                                                                                                       \n",
       "23573  Needed some light to maneuver around in the AM ,Someone tends to get crabby when I need a little light on to get ready for work and I turn lights on to move around the house,  So I am very satisfied with this product                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "23574  I should start by saying that I received this product for free from Toogou for testing and review.\\n\\nThis flashlight is made pretty well, it is all milled aluminum.  There is an O-ring on the end cap to help with water resistance.  It has 5 modes  high, medium, low, strobe and SOS.  All work well and the brightness levels are distinct to give you a nice range.  The button clicks nicely to toggle through all the modes but there is no mode memory so you have to toggle through every mode every time you use it, not uncommon for a flashlight of this price range.  There is no Lumen listing but I would guess the high level is about 70 lumens.  The beam spread is decent and even with no hot spot.  When zoomed out it comes to a small bright square spot.  The light feels good in hand.\\n\\nThis comes with 1 18650 rechargeable battery with tube adaptor as well as an insert so you can use 3 AAA batteries.  I tried it with both and didn't notice any difference in performance or brightness.  I would guess the difference would be battery life.  A charger for the 18650 battery is also included as well as a cheap lanyard.\\n\\nThe only real issue with this is that the zoom feature is a bit loose when extended.  It doesnt stay out well and wiggles a bit.  Other than that this is a good flashlight for the price.  It doesnt compare to higher end flashlights obviously but it is solid and the fact that they include the 18650 battery makes this worth the price.   \n",
       "23575  Nice well made wire wheel but not sized properly to fit a 4 1/2 grinder. I installed on my larger grinder and reordered  from another manufacturer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "23576  I do a fair amount of residential light switch upgrades and an occasional ceiling fan install. This is a nice tool to have in the tool kit and does make pulling the wires out it the box easier, stuffing the wires back into to the box is much more precise using this tool.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23577  Need some solder to repair some small diameter wires, the solder worked perfectly with no issues.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "       label  rank  \n",
       "0      0      1.0   \n",
       "1      0      1.0   \n",
       "2      0      1.0   \n",
       "3      0      1.0   \n",
       "4      0      1.0   \n",
       "...   ..      ...   \n",
       "23573  1      0.0   \n",
       "23574  1      0.0   \n",
       "23575  1      0.0   \n",
       "23576  1      0.0   \n",
       "23577  1      0.0   \n",
       "\n",
       "[23578 rows x 6 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = ['Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "# Initialize an empty list to store the DataFrames\n",
    "df_list = []\n",
    "\n",
    "for dataset in datasets: # iterate over the datasets\n",
    "    \n",
    "    df = pd.read_csv(os.getcwd()+'/results/'+dataset+'/balanced_labeled_dataset.csv', index_col=0) # load the balanced dataset \n",
    "\n",
    "    df_list.append(df) # list of dataframes\n",
    "\n",
    "# Vertically stack the DataFrames\n",
    "full_balanced_labeled_dataset = pd.concat(df_list, axis=0, ignore_index=False)\n",
    "full_balanced_labeled_dataset.reset_index(inplace=True)\n",
    "full_balanced_labeled_dataset.rename(columns={'index': 'Category_index'}, inplace=True)\n",
    "full_balanced_labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to a new CSV file if needed\n",
    "full_balanced_labeled_dataset.to_csv(os.getcwd()+'/results/full_balanced_labeled_dataset.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Noga1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
