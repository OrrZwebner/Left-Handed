{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro:\n",
    "Before running the script, make sure to donwload csv files from Amazon review data:\n",
    "https://nijianmo.github.io/amazon/index.html\n",
    "1. Create a folder in the same directory as this notebook named 'json_fies'\n",
    "2. Download and save the following json files:\n",
    "           reviews: 'Office_Products','Arts_Crafts_and_Sewing', 'Electronics', \n",
    "           'Home_and_Kitchen','Tools_and_Home_Improvement'. Make sure to save them in the same format\n",
    "               (for example: Office_Products.json)\n",
    "           5-core: 'Office_Products', 'Patio_Lawn_and_Garden'. Make sure to save them as Office_Products_5.json\n",
    "The first script:\n",
    "\n",
    "a. Converts the json files to CSV files\n",
    "\n",
    "b. Mark Reviews as \"left\" reviews according to chosen phrases\n",
    "\n",
    "c. Mark ID's as left ID's according to left reviews\n",
    "\n",
    "d. Create a dataset of left_ID reviews - reviews that were written by Left handed people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iBpYyDLzIVwu"
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "import pandas as pd\n",
    "# Array\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import sys,os,json\n",
    "from pathlib import Path\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import sys,os,json,csv\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Threading package\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Amazon JSON files to CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method only (will be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(path, name, cols):\n",
    "    \"\"\"\n",
    "    convert large json files to a csv file.\n",
    "    Input:\n",
    "    - path: (string) to the directory of the json file (including the file)\n",
    "    - name: (string) of the dataset\n",
    "    - cols: (list of strings) names of the cols to save as dataframe\n",
    "    Returns dataframe with the relevant columns\n",
    "    \"\"\"\n",
    "    print(f'start json_to_csv: {name}')\n",
    "    start_time = time.time() # calculate time of running\n",
    "    file = Path(path) # create a path to the json file\n",
    "    csv_file = open('csv_files/'+name+'.csv', 'w') # create and open a csv file named by the name of the dataset in the csv directory\n",
    "    csv_writer = csv.writer(csv_file) # create a csv write object\n",
    "#     cols.insert(0,'index')\n",
    "    csv_writer.writerow(cols) # create columns header to the csv file\n",
    "    line_count = 0\n",
    "    with file.open('r') as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            line_dict = json.loads(line) # load the row to a dictionary\n",
    "            line_list = [] # list of the row values. line_count - an index for the line\n",
    "            for key in cols:\n",
    "                if key not in line_dict: # add review only of it is exist\n",
    "                    line_dict[key] = \"\"\n",
    "                line_list.append(line_dict[key]) # add the current key to the line list        \n",
    "            csv_writer.writerow(line_list) # write the rows to the csv file\n",
    "   \n",
    "    print(f\"done converting \\\"{path}\\\" to \\\"csv_files/\" + name + f\".csv\\\", {line_count} lines \\nrun time: {time.time()-start_time:.3f} seconds\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews - object oriented methods' class (used for analizing the above csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews:\n",
    "    \n",
    "    def __init__(self, name, path, columns=['reviewerID','asin','overall','reviewText']):\n",
    "        \"\"\"\n",
    "        A class to represent a single reviews database from Amazon Review Data (2018) - https://nijianmo.github.io/amazon/index.html\n",
    "        \n",
    "        Input\n",
    "        - namet: The name of the Dataset\n",
    "        - path : Pass to the json file\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        \n",
    "        # columns that we slice out of all the columns\n",
    "        self.columns = columns \n",
    "        \n",
    "        # create a dataframe of all reviews (temporary because of size)\n",
    "        self.df_reviews = pd.DataFrame(data=None) \n",
    "        \n",
    "        #  sliced dataframes of reviews according to the chosen phrases\n",
    "        self.df_left_reviews = pd.DataFrame(data=None) \n",
    "        \n",
    "        #  Dataframe of left users ID's (ID, Number of left reviews)\n",
    "        self.df_lefties = pd.DataFrame(data=None) \n",
    "        \n",
    "        # dictionary of lists of adjacent words to the 'left' phrases\n",
    "            # the keys are phrases, values are 2 sorted lists (of tuples - word and number of counts. sorting accoring to counts) - before and after\n",
    "        self.adjacent_words = {} \n",
    "        \n",
    "        # reviews that were written by left IDs (the final output - ID's that we marked as written by lefties)\n",
    "        self.df_left_IDs_reviews = pd.DataFrame(data=None, columns = columns) \n",
    "        \n",
    "        # count number of total reviews for each left ID (total reviews in addtion to left review))\n",
    "        self.lefties_num_of_reviews = pd.DataFrame(data=None) \n",
    "        \n",
    "\n",
    "    def execute(self, phrases):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - phrase: the \"left handed\" phrase (string)\n",
    "        Run all the relevant functions on the current reviews dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # create a list of Concurrent Fututre objects\n",
    "        threads = [] \n",
    "        \n",
    "        # create a csv file\n",
    "        json_to_csv(path=self.path, name=self.name, cols=self.columns) \n",
    "        start = time.time()\n",
    "        \n",
    "        # iterate over the csv file in chunks\n",
    "        with pd.read_csv('csv_files/'+self.name+'.csv', chunksize=100000) as reader: \n",
    "            for chunk in reader:\n",
    "                # run analize data method over each chunk. analize data - create dataframe of left reviews\n",
    "                with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                    \n",
    "                    # assign execute function to thrd \n",
    "                    thrd = executor.submit(self.analize_data, df_reviews = chunk, phrases=phrases) \n",
    "                    \n",
    "                    # add a column of number of words in each review\n",
    "                    chunk = self.count_reviews_length(chunk) \n",
    "                    \n",
    "                    # append all the columns beside review_text to the attribute\n",
    "                    self.df_reviews = self.df_reviews.append(other=chunk.loc[:, chunk.columns != \"reviewText\"]) \n",
    "                    threads.append(thrd) \n",
    "        \n",
    "        counter = -1\n",
    "        \n",
    "        # print if thread is done, backup solution were not asked.\n",
    "        for thrd in threads: \n",
    "            if thrd.done():\n",
    "                counter += 1\n",
    "                print(f'chunk {counter} of dataset {self.name} is completed') # \n",
    "               \n",
    "            \n",
    "        # run additional functions. mark left reviews must be before count_save\n",
    "        \n",
    "        # find \"left\" IDs according to you string and create dataframe of lefties\n",
    "        self.find_left_IDs()\n",
    "        \n",
    "        #  Add/change features to total reviews (for each review) - left ID review/left review\n",
    "        self.mark_reviews() \n",
    "        \n",
    "        # Add number of review of lefties to self.df_lefties\n",
    "        self.count_save_lefties_total_reviews() \n",
    "        \n",
    "        # Find adjacent words to the \"left\" Phrases in each review and count them\n",
    "            # creates a dataframe of 2 cols:  phrases, and dictionary with 2 keys - before, after.)\n",
    "#         self.find_adjacent_words(phrases=phrases, threshold=10) \n",
    "\n",
    "        print(f'finished executing after {time.time() - start} seconds')\n",
    "    \n",
    "    def analize_data(self, df_reviews, phrases):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "        - df_reviews: (DataFrame) chunked reviews indlucding all features\n",
    "        - phrases: string/list of strings of phrases to analize\n",
    "        Fill the left's dataframes (self.df_left_reviews)\n",
    "        \"\"\"\n",
    "        \n",
    "        # lower the reviews and delete Punctuation\n",
    "        df_reviews['reviewText'] = df_reviews['reviewText'].str.lower().str.replace('[^\\w\\s]','') \n",
    "        \n",
    "        # if there is only 1 phrase - add  directly\n",
    "        if type(phrases) == str: \n",
    "            # convert the string to a list of 1 item\n",
    "            phrases = [phrases] \n",
    "\n",
    "        # iterate over all the phrases  \n",
    "        if type(phrases) == list: \n",
    "            for phrase in phrases:\n",
    "                self.df_left_reviews = self.df_left_reviews.append(df_reviews[df_reviews[\"reviewText\"].str.contains(phrase) == True], ignore_index=False) # slice the rows according to the phrase\n",
    "        \n",
    "        else:\n",
    "            print(f\"{phrases} is not a list of strings\")\n",
    "               \n",
    "        \n",
    "    def find_left_IDs(self):\n",
    "        \"\"\"\n",
    "        Fill the lefties dataframes (self.df_lefties)\n",
    "        \"\"\"\n",
    "        df_left_reviews = self.df_left_reviews\n",
    "        \n",
    "        # group by user ID and count number of rows \n",
    "        df_lefties = df_left_reviews.groupby(by='reviewerID', as_index=False).count() \n",
    "        \n",
    "        # count according to reviewText - uniqe attribute. n_left_review - number of left phrases (im left handed) reviews (by count method)\n",
    "        df_lefties = df_lefties.rename(columns={\"reviewText\": \"n_left_reviews\"}) \n",
    "        \n",
    "        # assign the dataframe to the field\n",
    "        self.df_lefties = df_lefties.loc[:, ['reviewerID',\"n_left_reviews\"]] \n",
    "    \n",
    "    \n",
    "    def count_save_lefties_total_reviews(self):\n",
    "        \"\"\"\n",
    "        count the number of reviews of a left ID and save it in an attribute (IDs and reviews)\n",
    "        Returns dataframe of Ids and number of reviews\n",
    "        \"\"\"\n",
    "        \n",
    "        lefties_num_of_reviews = pd.DataFrame(data=None, columns=['reviewerID', 'count'])\n",
    "        \n",
    "        # slice all the lefties IDs reviews from the total dataset (by their ID)\n",
    "        df_lefties_total_reviews = self.df_reviews[self.df_reviews['reviewerID'].isin(self.df_lefties['reviewerID'].unique())]\n",
    "        \n",
    "        # count number of repetitions, order by index in order to compare to df_lefties by ID\n",
    "        lefties_num_of_reviews = df_lefties_total_reviews['reviewerID'].value_counts().sort_index() \n",
    "        self.df_lefties = self.df_lefties.sort_values(by='reviewerID')\n",
    "        self.df_lefties['n_total_reviews'] = lefties_num_of_reviews.values\n",
    "        \n",
    "        # edit the attribute of reviews\n",
    "        self.df_left_IDs_reviews = df_lefties_total_reviews \n",
    "        \n",
    "        # keep the number of reviews for each left ID\n",
    "        self.lefties_num_of_reviews = lefties_num_of_reviews \n",
    "\n",
    "    \n",
    "    def mark_reviews(self):\n",
    "        \"\"\"\n",
    "        Add/change features to total reviews:\n",
    "        LeftReview - New feature. Mark left reviews (contain left phrases) with additional field (0/1) if it is a left review\n",
    "        Left_ID_Review - New feature Mark left ID reviews (written by a left ID) with additional field (0/1) if it is a left ID review\n",
    "        overall - New feature that converts the rating [a numerical feature (1-5) from the original review to an overall rating high(h)/medium(m), low(l)]\n",
    "        \"\"\"\n",
    "        # left reviews\n",
    "        \n",
    "        # create a column of zeros for marking the left reviews\n",
    "        left_reviews = np.zeros(shape=self.df_reviews.shape[0], dtype=int) \n",
    "        \n",
    "        # mark the left reviews as ones\n",
    "        left_reviews[self.df_reviews.index.isin(self.df_left_reviews.index)] = 1 \n",
    "        \n",
    "        # edit the LeftReviews column\n",
    "        self.df_reviews['LeftReview'] = left_reviews.astype('int32') \n",
    "        \n",
    "        \n",
    "        # left ID reviews\n",
    "        \n",
    "        # create a column of zeros for marking the left ID reviews\n",
    "        left_ID_reviews = np.zeros(shape=self.df_reviews.shape[0], dtype=int) \n",
    "        \n",
    "        # mark the left ID reviews as ones\n",
    "        left_ID_reviews[self.df_reviews['reviewerID'].isin(self.df_lefties['reviewerID'])] = 1 \n",
    "        \n",
    "        # edit the LeftReviews column\n",
    "        self.df_reviews['Left_ID_Review'] = left_ID_reviews.astype('int32') \n",
    "        \n",
    "        # convert overall to h/m/l (total, left and left_ID)\n",
    "        \n",
    "        # lambda function to convert rate into low, medium or high\n",
    "        convert_overal = lambda rate_num: 'h' if rate_num>3 else('l' if rate_num<3 else 'm') \n",
    "        self.df_reviews['overall'] = self.df_reviews['overall'].apply(convert_overal)\n",
    "        self.df_left_IDs_reviews['overall'] = self.df_left_IDs_reviews['overall'].apply(convert_overal)\n",
    "        self.df_left_reviews['overall'] = self.df_left_reviews['overall'].apply(convert_overal)\n",
    "  \n",
    "\n",
    "    def zeroize_df_reviews(self):\n",
    "        \"\"\"\n",
    "        delete df_reviews in order to keep space in the RAM\n",
    "        \"\"\"\n",
    "        self.df_reviews = None\n",
    "    \n",
    "    \n",
    "    def disribute_left_reviews_length(self):\n",
    "        \"\"\"\n",
    "        create plots of number of words per review\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6,4), dpi = 120)\n",
    "        sns.histplot(data=self.df_left_IDs_reviews, x ='n_words', kde=True);\n",
    "        plt.xlabel('number of words')\n",
    "        plt.ylabel('count')\n",
    "        plt.title(f'distribution of number of words per \\'left\\' review \\n{self.name}\\n', fontsize=12)\n",
    "        plt.show()\n",
    "    \n",
    "    def count_reviews_length(self, chunk):\n",
    "        \"\"\"\n",
    "        count the number of words in the each review in the chunk.\n",
    "        add it as an attribute to the chunk.\n",
    "        \"\"\"\n",
    "        \n",
    "        # counter number of words\n",
    "        chunk['n_words'] = chunk[\"reviewText\"].str.split().str.len() \n",
    "        \n",
    "        # convert to int\n",
    "        chunk['n_words'] = chunk['n_words'].astype(dtype='int32', errors = 'ignore') \n",
    "        return chunk\n",
    "    \n",
    "    \n",
    "    def export_results(self, threshold=0, left_hand_results = True, str_hand = None):\n",
    "        \"\"\"\n",
    "        export the results to csv files.\n",
    "        Input:\n",
    "        :param threshold (int) - minimum number of words for review\n",
    "        :param left_hand_results (boolean) - is it running for left reviews or a new research...\n",
    "        \"\"\"\n",
    "        \n",
    "        # create a path to the results folder\n",
    "        path = os.path.join(os.getcwd(), 'results/'+self.name) \n",
    "        \n",
    "        # if the directory doesn't exist, create it\n",
    "        if not os.path.exists(path): \n",
    "            os.mkdir(path)\n",
    "            \n",
    "        if left_hand_results:    \n",
    "            self.df_lefties.to_csv(path + '/left_ID\\'s.csv')\n",
    "            self.df_left_reviews.to_csv(path + '/left_reviews.csv')\n",
    "            self.df_reviews.to_csv(path + '/total_reviews.csv')\n",
    "\n",
    "            # csv of left ID reviews\n",
    "            df_left_IDs_reviews = self.df_left_IDs_reviews[self.df_left_IDs_reviews['n_words'] > threshold] \n",
    "            df_left_IDs_reviews = df_left_IDs_reviews[df_left_IDs_reviews['LeftReview'] != 1] # remove the left phrase reviews ('im left handed etc.')\n",
    "            df_left_IDs_reviews.to_csv(path + '/left_ID_reviews.csv')\n",
    " \n",
    "        # for other datasets such as right handed\n",
    "        if not left_hand_results: \n",
    "            self.df_lefties.to_csv(path + '/' + str_hand + '_ID\\'s.csv')\n",
    "            self.df_left_reviews.to_csv(path + '/' + str_hand + '_reviews.csv')\n",
    "            # csv of sliced ID reviews\n",
    "            df_left_IDs_reviews = self.df_left_IDs_reviews[self.df_left_IDs_reviews['n_words'] > threshold] \n",
    "            \n",
    "            # remove the left phrase reviews ('im left handed etc.')\n",
    "            df_left_IDs_reviews = df_left_IDs_reviews[df_left_IDs_reviews['LeftReview'] != 1] \n",
    "            df_left_IDs_reviews.to_csv(path + '/' + str_hand + '_ID_reviews.csv')\n",
    "            \n",
    "    \n",
    "    def find_adjacent_words(self, phrases, threshold = 10):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - reviews: list of reviews\n",
    "        - phrases: the \"left handed\" phrases (list of strings)\n",
    "        - threshold: number of most common words to list(integer)\n",
    "        Returns a dataframe of 2 cols:  phrases, and dictionary with 2 keys - before, after.\n",
    "        Each key has as a value  a list of the  10 (or a diffrent threshold) most common adjacent words to each phrase according to the direction (before or after).\n",
    "        The list is built by counter object - container library.\n",
    "        \"\"\"\n",
    "        \n",
    "        # if there is only 1 phrase - add it to the dictionary\n",
    "        if type(phrases) == str: \n",
    "            \n",
    "            # convert it to a list of 1 element\n",
    "            phrases = [phrases] \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        for phrase in phrases:\n",
    "        \n",
    "        # check if there is a dataframe sliced according to the phrase\n",
    "            if  self.df_left_reviews[phrase].empty: \n",
    "                print(f\"no relevant dataframe according to the phrase {phrase}\")\n",
    "                continue\n",
    "                \n",
    "            # list of the first words before the phrase    \n",
    "            one_word_before = [] \n",
    "            \n",
    "            # list of the first words before the phrase\n",
    "            one_word_after = []  \n",
    "            \n",
    "            # list of reviews according to the phrase (self.df_left_reviews - dictionary/dataframe) \n",
    "            reviews = self.df_left_reviews['reviewText'] \n",
    "            \n",
    "            # iterate over all the reviews\n",
    "            for review in reviews: \n",
    "                # split the review where the phrase is\n",
    "                splitted_review = review.split(phrase) \n",
    "                \n",
    "                # only iterate over reviews that contain the given phrase (if the length is 1 or smaller - there was not split)\n",
    "                if len(splitted_review) < 2: \n",
    "                    continue\n",
    "\n",
    "                # iterate over the splitted parts of the review\n",
    "                for i in range(len(splitted_review)): \n",
    "                    \n",
    "                    # only check parts that contain string (words)\n",
    "                    if(splitted_review[i].strip()): \n",
    "                        \n",
    "                        # the phrase was after this part (if there is more than 1 phrase in this review - all the even parts are before)  \n",
    "                        if i%2 == 0:\n",
    "                            \n",
    "                            # the last word was adjacent from the left\n",
    "                            one_word_before.append(splitted_review[i].split()[-1]) \n",
    "                            \n",
    "                        # the phrase was before this part \n",
    "                        else: \n",
    "                            \n",
    "                            # the first word was adjacent from the right\n",
    "                            one_word_after.append(splitted_review[i].split()[0]) \n",
    "                            \n",
    "                # dictionary with 2 sorted lists (of tuples - word and number of counts) - before and after        \n",
    "                self.adjacent_words[phrase] = {'before': Counter(one_word_before).most_common(threshold), 'after': Counter(one_word_after).most_common(threshold)} \n",
    "                \n",
    "                print(f'{self.name}:\\n{phrase} \\none_word_before: {self.adjacent_words[phrase][\"before\"]}),\\none_word_after: {self.adjacent_words[phrase][\"after\"]}')\n",
    "        \n",
    "        return self.adjacent_words\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset of left handed using 2 phrases - 'im left handed', 'i am left handed'. Sainity test over a small Amazon dataset ('Office_Products_5')\n",
    "Reviews removes puncutation and lower case im=Im=i'm=I'm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start json_to_csv: Patio_Lawn_and_Garden_5\n",
      "done converting \"json_files/Patio_Lawn_and_Garden_5.json\" to \"csv_files/Patio_Lawn_and_Garden_5.csv\", 798415 lines \n",
      "run time: 13.355 seconds\n",
      "chunk 0 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 1 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 2 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 3 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 4 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 5 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 6 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "chunk 7 of dataset Patio_Lawn_and_Garden_5 is completed\n",
      "finished executing after 19.143455982208252 seconds\n"
     ]
    }
   ],
   "source": [
    "reviews_dataset = Reviews(name='Office_Products_5', path='json_files/Patio_Lawn_and_Garden_5.json', columns=['reviewerID','asin','overall','reviewText'] )\n",
    "phrases = ['im left handed', 'i am left handed'] \n",
    "reviews_dataset.execute(phrases=phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Export the sainity test results (sliced reviews accordign to the above phrses) to new csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dataset.export_results(threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'results/'+reviews_dataset.name) # create a path to the results folder\n",
    "reviews_dataset.df_reviews.to_csv(path + '/total_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset of left handed IDs using 2 phrases - 'im left handed', 'i am left handed' over 5 full amazon reviews datasets - 'Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start json_to_csv: Office_Products\n",
      "\n",
      "finished threading \"Office_Products\" after 0.00 seconds\n",
      "done converting \"json_files/Office_Products.json\" to \"csv_files/Office_Products.csv\", 5581313 lines \n",
      "run time: 46.295 seconds\n",
      "chunk 0 of dataset Office_Products is completed\n",
      "chunk 1 of dataset Office_Products is completed\n",
      "chunk 2 of dataset Office_Products is completed\n",
      "chunk 3 of dataset Office_Products is completed\n",
      "chunk 4 of dataset Office_Products is completed\n",
      "chunk 5 of dataset Office_Products is completed\n",
      "chunk 6 of dataset Office_Products is completed\n",
      "chunk 7 of dataset Office_Products is completed\n",
      "chunk 8 of dataset Office_Products is completed\n",
      "chunk 9 of dataset Office_Products is completed\n",
      "chunk 10 of dataset Office_Products is completed\n",
      "chunk 11 of dataset Office_Products is completed\n",
      "chunk 12 of dataset Office_Products is completed\n",
      "chunk 13 of dataset Office_Products is completed\n",
      "chunk 14 of dataset Office_Products is completed\n",
      "chunk 15 of dataset Office_Products is completed\n",
      "chunk 16 of dataset Office_Products is completed\n",
      "chunk 17 of dataset Office_Products is completed\n",
      "chunk 18 of dataset Office_Products is completed\n",
      "chunk 19 of dataset Office_Products is completed\n",
      "chunk 20 of dataset Office_Products is completed\n",
      "chunk 21 of dataset Office_Products is completed\n",
      "chunk 22 of dataset Office_Products is completed\n",
      "chunk 23 of dataset Office_Products is completed\n",
      "chunk 24 of dataset Office_Products is completed\n",
      "chunk 25 of dataset Office_Products is completed\n",
      "chunk 26 of dataset Office_Products is completed\n",
      "chunk 27 of dataset Office_Products is completed\n",
      "chunk 28 of dataset Office_Products is completed\n",
      "chunk 29 of dataset Office_Products is completed\n",
      "chunk 30 of dataset Office_Products is completed\n",
      "chunk 31 of dataset Office_Products is completed\n",
      "chunk 32 of dataset Office_Products is completed\n",
      "chunk 33 of dataset Office_Products is completed\n",
      "chunk 34 of dataset Office_Products is completed\n",
      "chunk 35 of dataset Office_Products is completed\n",
      "chunk 36 of dataset Office_Products is completed\n",
      "chunk 37 of dataset Office_Products is completed\n",
      "chunk 38 of dataset Office_Products is completed\n",
      "chunk 39 of dataset Office_Products is completed\n",
      "chunk 40 of dataset Office_Products is completed\n",
      "chunk 41 of dataset Office_Products is completed\n",
      "chunk 42 of dataset Office_Products is completed\n",
      "chunk 43 of dataset Office_Products is completed\n",
      "chunk 44 of dataset Office_Products is completed\n",
      "chunk 45 of dataset Office_Products is completed\n",
      "chunk 46 of dataset Office_Products is completed\n",
      "chunk 47 of dataset Office_Products is completed\n",
      "chunk 48 of dataset Office_Products is completed\n",
      "chunk 49 of dataset Office_Products is completed\n",
      "chunk 50 of dataset Office_Products is completed\n",
      "chunk 51 of dataset Office_Products is completed\n",
      "chunk 52 of dataset Office_Products is completed\n",
      "chunk 53 of dataset Office_Products is completed\n",
      "chunk 54 of dataset Office_Products is completed\n",
      "chunk 55 of dataset Office_Products is completed\n",
      "finished executing after 75.33044981956482 seconds\n",
      "start json_to_csv: Arts_Crafts_and_Sewing\n",
      "finished threading \"Arts_Crafts_and_Sewing\" after 0.00 seconds\n",
      "\n",
      "done converting \"json_files/Arts_Crafts_and_Sewing.json\" to \"csv_files/Arts_Crafts_and_Sewing.csv\", 2875917 lines \n",
      "run time: 21.742 seconds\n",
      "chunk 0 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 1 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 2 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 3 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 4 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 5 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 6 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 7 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 8 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 9 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 10 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 11 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 12 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 13 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 14 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 15 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 16 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 17 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 18 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 19 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 20 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 21 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 22 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 23 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 24 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 25 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 26 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 27 of dataset Arts_Crafts_and_Sewing is completed\n",
      "chunk 28 of dataset Arts_Crafts_and_Sewing is completed\n",
      "finished executing after 31.67713212966919 seconds\n",
      "start json_to_csv: Electronics\n",
      "\n",
      "finished threading \"Electronics\" after 0.00 seconds\n",
      "done converting \"json_files/Electronics.json\" to \"csv_files/Electronics.csv\", 20994353 lines \n",
      "run time: 209.957 seconds\n",
      "chunk 0 of dataset Electronics is completed\n",
      "chunk 1 of dataset Electronics is completed\n",
      "chunk 2 of dataset Electronics is completed\n",
      "chunk 3 of dataset Electronics is completed\n",
      "chunk 4 of dataset Electronics is completed\n",
      "chunk 5 of dataset Electronics is completed\n",
      "chunk 6 of dataset Electronics is completed\n",
      "chunk 7 of dataset Electronics is completed\n",
      "chunk 8 of dataset Electronics is completed\n",
      "chunk 9 of dataset Electronics is completed\n",
      "chunk 10 of dataset Electronics is completed\n",
      "chunk 11 of dataset Electronics is completed\n",
      "chunk 12 of dataset Electronics is completed\n",
      "chunk 13 of dataset Electronics is completed\n",
      "chunk 14 of dataset Electronics is completed\n",
      "chunk 15 of dataset Electronics is completed\n",
      "chunk 16 of dataset Electronics is completed\n",
      "chunk 17 of dataset Electronics is completed\n",
      "chunk 18 of dataset Electronics is completed\n",
      "chunk 19 of dataset Electronics is completed\n",
      "chunk 20 of dataset Electronics is completed\n",
      "chunk 21 of dataset Electronics is completed\n",
      "chunk 22 of dataset Electronics is completed\n",
      "chunk 23 of dataset Electronics is completed\n",
      "chunk 24 of dataset Electronics is completed\n",
      "chunk 25 of dataset Electronics is completed\n",
      "chunk 26 of dataset Electronics is completed\n",
      "chunk 27 of dataset Electronics is completed\n",
      "chunk 28 of dataset Electronics is completed\n",
      "chunk 29 of dataset Electronics is completed\n",
      "chunk 30 of dataset Electronics is completed\n",
      "chunk 31 of dataset Electronics is completed\n",
      "chunk 32 of dataset Electronics is completed\n",
      "chunk 33 of dataset Electronics is completed\n",
      "chunk 34 of dataset Electronics is completed\n",
      "chunk 35 of dataset Electronics is completed\n",
      "chunk 36 of dataset Electronics is completed\n",
      "chunk 37 of dataset Electronics is completed\n",
      "chunk 38 of dataset Electronics is completed\n",
      "chunk 39 of dataset Electronics is completed\n",
      "chunk 40 of dataset Electronics is completed\n",
      "chunk 41 of dataset Electronics is completed\n",
      "chunk 42 of dataset Electronics is completed\n",
      "chunk 43 of dataset Electronics is completed\n",
      "chunk 44 of dataset Electronics is completed\n",
      "chunk 45 of dataset Electronics is completed\n",
      "chunk 46 of dataset Electronics is completed\n",
      "chunk 47 of dataset Electronics is completed\n",
      "chunk 48 of dataset Electronics is completed\n",
      "chunk 49 of dataset Electronics is completed\n",
      "chunk 50 of dataset Electronics is completed\n",
      "chunk 51 of dataset Electronics is completed\n",
      "chunk 52 of dataset Electronics is completed\n",
      "chunk 53 of dataset Electronics is completed\n",
      "chunk 54 of dataset Electronics is completed\n",
      "chunk 55 of dataset Electronics is completed\n",
      "chunk 56 of dataset Electronics is completed\n",
      "chunk 57 of dataset Electronics is completed\n",
      "chunk 58 of dataset Electronics is completed\n",
      "chunk 59 of dataset Electronics is completed\n",
      "chunk 60 of dataset Electronics is completed\n",
      "chunk 61 of dataset Electronics is completed\n",
      "chunk 62 of dataset Electronics is completed\n",
      "chunk 63 of dataset Electronics is completed\n",
      "chunk 64 of dataset Electronics is completed\n",
      "chunk 65 of dataset Electronics is completed\n",
      "chunk 66 of dataset Electronics is completed\n",
      "chunk 67 of dataset Electronics is completed\n",
      "chunk 68 of dataset Electronics is completed\n",
      "chunk 69 of dataset Electronics is completed\n",
      "chunk 70 of dataset Electronics is completed\n",
      "chunk 71 of dataset Electronics is completed\n",
      "chunk 72 of dataset Electronics is completed\n",
      "chunk 73 of dataset Electronics is completed\n",
      "chunk 74 of dataset Electronics is completed\n",
      "chunk 75 of dataset Electronics is completed\n",
      "chunk 76 of dataset Electronics is completed\n",
      "chunk 77 of dataset Electronics is completed\n",
      "chunk 78 of dataset Electronics is completed\n",
      "chunk 79 of dataset Electronics is completed\n",
      "chunk 80 of dataset Electronics is completed\n",
      "chunk 81 of dataset Electronics is completed\n",
      "chunk 82 of dataset Electronics is completed\n",
      "chunk 83 of dataset Electronics is completed\n",
      "chunk 84 of dataset Electronics is completed\n",
      "chunk 85 of dataset Electronics is completed\n",
      "chunk 86 of dataset Electronics is completed\n",
      "chunk 87 of dataset Electronics is completed\n",
      "chunk 88 of dataset Electronics is completed\n",
      "chunk 89 of dataset Electronics is completed\n",
      "chunk 90 of dataset Electronics is completed\n",
      "chunk 91 of dataset Electronics is completed\n",
      "chunk 92 of dataset Electronics is completed\n",
      "chunk 93 of dataset Electronics is completed\n",
      "chunk 94 of dataset Electronics is completed\n",
      "chunk 95 of dataset Electronics is completed\n",
      "chunk 96 of dataset Electronics is completed\n",
      "chunk 97 of dataset Electronics is completed\n",
      "chunk 98 of dataset Electronics is completed\n",
      "chunk 99 of dataset Electronics is completed\n",
      "chunk 100 of dataset Electronics is completed\n",
      "chunk 101 of dataset Electronics is completed\n",
      "chunk 102 of dataset Electronics is completed\n",
      "chunk 103 of dataset Electronics is completed\n",
      "chunk 104 of dataset Electronics is completed\n",
      "chunk 105 of dataset Electronics is completed\n",
      "chunk 106 of dataset Electronics is completed\n",
      "chunk 107 of dataset Electronics is completed\n",
      "chunk 108 of dataset Electronics is completed\n",
      "chunk 109 of dataset Electronics is completed\n",
      "chunk 110 of dataset Electronics is completed\n",
      "chunk 111 of dataset Electronics is completed\n",
      "chunk 112 of dataset Electronics is completed\n",
      "chunk 113 of dataset Electronics is completed\n",
      "chunk 114 of dataset Electronics is completed\n",
      "chunk 115 of dataset Electronics is completed\n",
      "chunk 116 of dataset Electronics is completed\n",
      "chunk 117 of dataset Electronics is completed\n",
      "chunk 118 of dataset Electronics is completed\n",
      "chunk 119 of dataset Electronics is completed\n",
      "chunk 120 of dataset Electronics is completed\n",
      "chunk 121 of dataset Electronics is completed\n",
      "chunk 122 of dataset Electronics is completed\n",
      "chunk 123 of dataset Electronics is completed\n",
      "chunk 124 of dataset Electronics is completed\n",
      "chunk 125 of dataset Electronics is completed\n",
      "chunk 126 of dataset Electronics is completed\n",
      "chunk 127 of dataset Electronics is completed\n",
      "chunk 128 of dataset Electronics is completed\n",
      "chunk 129 of dataset Electronics is completed\n",
      "chunk 130 of dataset Electronics is completed\n",
      "chunk 131 of dataset Electronics is completed\n",
      "chunk 132 of dataset Electronics is completed\n",
      "chunk 133 of dataset Electronics is completed\n",
      "chunk 134 of dataset Electronics is completed\n",
      "chunk 135 of dataset Electronics is completed\n",
      "chunk 136 of dataset Electronics is completed\n",
      "chunk 137 of dataset Electronics is completed\n",
      "chunk 138 of dataset Electronics is completed\n",
      "chunk 139 of dataset Electronics is completed\n",
      "chunk 140 of dataset Electronics is completed\n",
      "chunk 141 of dataset Electronics is completed\n",
      "chunk 142 of dataset Electronics is completed\n",
      "chunk 143 of dataset Electronics is completed\n",
      "chunk 144 of dataset Electronics is completed\n",
      "chunk 145 of dataset Electronics is completed\n",
      "chunk 146 of dataset Electronics is completed\n",
      "chunk 147 of dataset Electronics is completed\n",
      "chunk 148 of dataset Electronics is completed\n",
      "chunk 149 of dataset Electronics is completed\n",
      "chunk 150 of dataset Electronics is completed\n",
      "chunk 151 of dataset Electronics is completed\n",
      "chunk 152 of dataset Electronics is completed\n",
      "chunk 153 of dataset Electronics is completed\n",
      "chunk 154 of dataset Electronics is completed\n",
      "chunk 155 of dataset Electronics is completed\n",
      "chunk 156 of dataset Electronics is completed\n",
      "chunk 157 of dataset Electronics is completed\n",
      "chunk 158 of dataset Electronics is completed\n",
      "chunk 159 of dataset Electronics is completed\n",
      "chunk 160 of dataset Electronics is completed\n",
      "chunk 161 of dataset Electronics is completed\n",
      "chunk 162 of dataset Electronics is completed\n",
      "chunk 163 of dataset Electronics is completed\n",
      "chunk 164 of dataset Electronics is completed\n",
      "chunk 165 of dataset Electronics is completed\n",
      "chunk 166 of dataset Electronics is completed\n",
      "chunk 167 of dataset Electronics is completed\n",
      "chunk 168 of dataset Electronics is completed\n",
      "chunk 169 of dataset Electronics is completed\n",
      "chunk 170 of dataset Electronics is completed\n",
      "chunk 171 of dataset Electronics is completed\n",
      "chunk 172 of dataset Electronics is completed\n",
      "chunk 173 of dataset Electronics is completed\n",
      "chunk 174 of dataset Electronics is completed\n",
      "chunk 175 of dataset Electronics is completed\n",
      "chunk 176 of dataset Electronics is completed\n",
      "chunk 177 of dataset Electronics is completed\n",
      "chunk 178 of dataset Electronics is completed\n",
      "chunk 179 of dataset Electronics is completed\n",
      "chunk 180 of dataset Electronics is completed\n",
      "chunk 181 of dataset Electronics is completed\n",
      "chunk 182 of dataset Electronics is completed\n",
      "chunk 183 of dataset Electronics is completed\n",
      "chunk 184 of dataset Electronics is completed\n",
      "chunk 185 of dataset Electronics is completed\n",
      "chunk 186 of dataset Electronics is completed\n",
      "chunk 187 of dataset Electronics is completed\n",
      "chunk 188 of dataset Electronics is completed\n",
      "chunk 189 of dataset Electronics is completed\n",
      "chunk 190 of dataset Electronics is completed\n",
      "chunk 191 of dataset Electronics is completed\n",
      "chunk 192 of dataset Electronics is completed\n",
      "chunk 193 of dataset Electronics is completed\n",
      "chunk 194 of dataset Electronics is completed\n",
      "chunk 195 of dataset Electronics is completed\n",
      "chunk 196 of dataset Electronics is completed\n",
      "chunk 197 of dataset Electronics is completed\n",
      "chunk 198 of dataset Electronics is completed\n",
      "chunk 199 of dataset Electronics is completed\n",
      "chunk 200 of dataset Electronics is completed\n",
      "chunk 201 of dataset Electronics is completed\n",
      "chunk 202 of dataset Electronics is completed\n",
      "chunk 203 of dataset Electronics is completed\n",
      "chunk 204 of dataset Electronics is completed\n",
      "chunk 205 of dataset Electronics is completed\n",
      "chunk 206 of dataset Electronics is completed\n",
      "chunk 207 of dataset Electronics is completed\n",
      "chunk 208 of dataset Electronics is completed\n",
      "chunk 209 of dataset Electronics is completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished executing after 393.0843970775604 seconds\n",
      "start json_to_csv: Home_and_Kitchen\n",
      "finished threading \"Home_and_Kitchen\" after 0.00 seconds\n",
      "\n",
      "done converting \"json_files/Home_and_Kitchen.json\" to \"csv_files/Home_and_Kitchen.csv\", 21928568 lines \n",
      "run time: 184.648 seconds\n",
      "chunk 0 of dataset Home_and_Kitchen is completed\n",
      "chunk 1 of dataset Home_and_Kitchen is completed\n",
      "chunk 2 of dataset Home_and_Kitchen is completed\n",
      "chunk 3 of dataset Home_and_Kitchen is completed\n",
      "chunk 4 of dataset Home_and_Kitchen is completed\n",
      "chunk 5 of dataset Home_and_Kitchen is completed\n",
      "chunk 6 of dataset Home_and_Kitchen is completed\n",
      "chunk 7 of dataset Home_and_Kitchen is completed\n",
      "chunk 8 of dataset Home_and_Kitchen is completed\n",
      "chunk 9 of dataset Home_and_Kitchen is completed\n",
      "chunk 10 of dataset Home_and_Kitchen is completed\n",
      "chunk 11 of dataset Home_and_Kitchen is completed\n",
      "chunk 12 of dataset Home_and_Kitchen is completed\n",
      "chunk 13 of dataset Home_and_Kitchen is completed\n",
      "chunk 14 of dataset Home_and_Kitchen is completed\n",
      "chunk 15 of dataset Home_and_Kitchen is completed\n",
      "chunk 16 of dataset Home_and_Kitchen is completed\n",
      "chunk 17 of dataset Home_and_Kitchen is completed\n",
      "chunk 18 of dataset Home_and_Kitchen is completed\n",
      "chunk 19 of dataset Home_and_Kitchen is completed\n",
      "chunk 20 of dataset Home_and_Kitchen is completed\n",
      "chunk 21 of dataset Home_and_Kitchen is completed\n",
      "chunk 22 of dataset Home_and_Kitchen is completed\n",
      "chunk 23 of dataset Home_and_Kitchen is completed\n",
      "chunk 24 of dataset Home_and_Kitchen is completed\n",
      "chunk 25 of dataset Home_and_Kitchen is completed\n",
      "chunk 26 of dataset Home_and_Kitchen is completed\n",
      "chunk 27 of dataset Home_and_Kitchen is completed\n",
      "chunk 28 of dataset Home_and_Kitchen is completed\n",
      "chunk 29 of dataset Home_and_Kitchen is completed\n",
      "chunk 30 of dataset Home_and_Kitchen is completed\n",
      "chunk 31 of dataset Home_and_Kitchen is completed\n",
      "chunk 32 of dataset Home_and_Kitchen is completed\n",
      "chunk 33 of dataset Home_and_Kitchen is completed\n",
      "chunk 34 of dataset Home_and_Kitchen is completed\n",
      "chunk 35 of dataset Home_and_Kitchen is completed\n",
      "chunk 36 of dataset Home_and_Kitchen is completed\n",
      "chunk 37 of dataset Home_and_Kitchen is completed\n",
      "chunk 38 of dataset Home_and_Kitchen is completed\n",
      "chunk 39 of dataset Home_and_Kitchen is completed\n",
      "chunk 40 of dataset Home_and_Kitchen is completed\n",
      "chunk 41 of dataset Home_and_Kitchen is completed\n",
      "chunk 42 of dataset Home_and_Kitchen is completed\n",
      "chunk 43 of dataset Home_and_Kitchen is completed\n",
      "chunk 44 of dataset Home_and_Kitchen is completed\n",
      "chunk 45 of dataset Home_and_Kitchen is completed\n",
      "chunk 46 of dataset Home_and_Kitchen is completed\n",
      "chunk 47 of dataset Home_and_Kitchen is completed\n",
      "chunk 48 of dataset Home_and_Kitchen is completed\n",
      "chunk 49 of dataset Home_and_Kitchen is completed\n",
      "chunk 50 of dataset Home_and_Kitchen is completed\n",
      "chunk 51 of dataset Home_and_Kitchen is completed\n",
      "chunk 52 of dataset Home_and_Kitchen is completed\n",
      "chunk 53 of dataset Home_and_Kitchen is completed\n",
      "chunk 54 of dataset Home_and_Kitchen is completed\n",
      "chunk 55 of dataset Home_and_Kitchen is completed\n",
      "chunk 56 of dataset Home_and_Kitchen is completed\n",
      "chunk 57 of dataset Home_and_Kitchen is completed\n",
      "chunk 58 of dataset Home_and_Kitchen is completed\n",
      "chunk 59 of dataset Home_and_Kitchen is completed\n",
      "chunk 60 of dataset Home_and_Kitchen is completed\n",
      "chunk 61 of dataset Home_and_Kitchen is completed\n",
      "chunk 62 of dataset Home_and_Kitchen is completed\n",
      "chunk 63 of dataset Home_and_Kitchen is completed\n",
      "chunk 64 of dataset Home_and_Kitchen is completed\n",
      "chunk 65 of dataset Home_and_Kitchen is completed\n",
      "chunk 66 of dataset Home_and_Kitchen is completed\n",
      "chunk 67 of dataset Home_and_Kitchen is completed\n",
      "chunk 68 of dataset Home_and_Kitchen is completed\n",
      "chunk 69 of dataset Home_and_Kitchen is completed\n",
      "chunk 70 of dataset Home_and_Kitchen is completed\n",
      "chunk 71 of dataset Home_and_Kitchen is completed\n",
      "chunk 72 of dataset Home_and_Kitchen is completed\n",
      "chunk 73 of dataset Home_and_Kitchen is completed\n",
      "chunk 74 of dataset Home_and_Kitchen is completed\n",
      "chunk 75 of dataset Home_and_Kitchen is completed\n",
      "chunk 76 of dataset Home_and_Kitchen is completed\n",
      "chunk 77 of dataset Home_and_Kitchen is completed\n",
      "chunk 78 of dataset Home_and_Kitchen is completed\n",
      "chunk 79 of dataset Home_and_Kitchen is completed\n",
      "chunk 80 of dataset Home_and_Kitchen is completed\n",
      "chunk 81 of dataset Home_and_Kitchen is completed\n",
      "chunk 82 of dataset Home_and_Kitchen is completed\n",
      "chunk 83 of dataset Home_and_Kitchen is completed\n",
      "chunk 84 of dataset Home_and_Kitchen is completed\n",
      "chunk 85 of dataset Home_and_Kitchen is completed\n",
      "chunk 86 of dataset Home_and_Kitchen is completed\n",
      "chunk 87 of dataset Home_and_Kitchen is completed\n",
      "chunk 88 of dataset Home_and_Kitchen is completed\n",
      "chunk 89 of dataset Home_and_Kitchen is completed\n",
      "chunk 90 of dataset Home_and_Kitchen is completed\n",
      "chunk 91 of dataset Home_and_Kitchen is completed\n",
      "chunk 92 of dataset Home_and_Kitchen is completed\n",
      "chunk 93 of dataset Home_and_Kitchen is completed\n",
      "chunk 94 of dataset Home_and_Kitchen is completed\n",
      "chunk 95 of dataset Home_and_Kitchen is completed\n",
      "chunk 96 of dataset Home_and_Kitchen is completed\n",
      "chunk 97 of dataset Home_and_Kitchen is completed\n",
      "chunk 98 of dataset Home_and_Kitchen is completed\n",
      "chunk 99 of dataset Home_and_Kitchen is completed\n",
      "chunk 100 of dataset Home_and_Kitchen is completed\n",
      "chunk 101 of dataset Home_and_Kitchen is completed\n",
      "chunk 102 of dataset Home_and_Kitchen is completed\n",
      "chunk 103 of dataset Home_and_Kitchen is completed\n",
      "chunk 104 of dataset Home_and_Kitchen is completed\n",
      "chunk 105 of dataset Home_and_Kitchen is completed\n",
      "chunk 106 of dataset Home_and_Kitchen is completed\n",
      "chunk 107 of dataset Home_and_Kitchen is completed\n",
      "chunk 108 of dataset Home_and_Kitchen is completed\n",
      "chunk 109 of dataset Home_and_Kitchen is completed\n",
      "chunk 110 of dataset Home_and_Kitchen is completed\n",
      "chunk 111 of dataset Home_and_Kitchen is completed\n",
      "chunk 112 of dataset Home_and_Kitchen is completed\n",
      "chunk 113 of dataset Home_and_Kitchen is completed\n",
      "chunk 114 of dataset Home_and_Kitchen is completed\n",
      "chunk 115 of dataset Home_and_Kitchen is completed\n",
      "chunk 116 of dataset Home_and_Kitchen is completed\n",
      "chunk 117 of dataset Home_and_Kitchen is completed\n",
      "chunk 118 of dataset Home_and_Kitchen is completed\n",
      "chunk 119 of dataset Home_and_Kitchen is completed\n",
      "chunk 120 of dataset Home_and_Kitchen is completed\n",
      "chunk 121 of dataset Home_and_Kitchen is completed\n",
      "chunk 122 of dataset Home_and_Kitchen is completed\n",
      "chunk 123 of dataset Home_and_Kitchen is completed\n",
      "chunk 124 of dataset Home_and_Kitchen is completed\n",
      "chunk 125 of dataset Home_and_Kitchen is completed\n",
      "chunk 126 of dataset Home_and_Kitchen is completed\n",
      "chunk 127 of dataset Home_and_Kitchen is completed\n",
      "chunk 128 of dataset Home_and_Kitchen is completed\n",
      "chunk 129 of dataset Home_and_Kitchen is completed\n",
      "chunk 130 of dataset Home_and_Kitchen is completed\n",
      "chunk 131 of dataset Home_and_Kitchen is completed\n",
      "chunk 132 of dataset Home_and_Kitchen is completed\n",
      "chunk 133 of dataset Home_and_Kitchen is completed\n",
      "chunk 134 of dataset Home_and_Kitchen is completed\n",
      "chunk 135 of dataset Home_and_Kitchen is completed\n",
      "chunk 136 of dataset Home_and_Kitchen is completed\n",
      "chunk 137 of dataset Home_and_Kitchen is completed\n",
      "chunk 138 of dataset Home_and_Kitchen is completed\n",
      "chunk 139 of dataset Home_and_Kitchen is completed\n",
      "chunk 140 of dataset Home_and_Kitchen is completed\n",
      "chunk 141 of dataset Home_and_Kitchen is completed\n",
      "chunk 142 of dataset Home_and_Kitchen is completed\n",
      "chunk 143 of dataset Home_and_Kitchen is completed\n",
      "chunk 144 of dataset Home_and_Kitchen is completed\n",
      "chunk 145 of dataset Home_and_Kitchen is completed\n",
      "chunk 146 of dataset Home_and_Kitchen is completed\n",
      "chunk 147 of dataset Home_and_Kitchen is completed\n",
      "chunk 148 of dataset Home_and_Kitchen is completed\n",
      "chunk 149 of dataset Home_and_Kitchen is completed\n",
      "chunk 150 of dataset Home_and_Kitchen is completed\n",
      "chunk 151 of dataset Home_and_Kitchen is completed\n",
      "chunk 152 of dataset Home_and_Kitchen is completed\n",
      "chunk 153 of dataset Home_and_Kitchen is completed\n",
      "chunk 154 of dataset Home_and_Kitchen is completed\n",
      "chunk 155 of dataset Home_and_Kitchen is completed\n",
      "chunk 156 of dataset Home_and_Kitchen is completed\n",
      "chunk 157 of dataset Home_and_Kitchen is completed\n",
      "chunk 158 of dataset Home_and_Kitchen is completed\n",
      "chunk 159 of dataset Home_and_Kitchen is completed\n",
      "chunk 160 of dataset Home_and_Kitchen is completed\n",
      "chunk 161 of dataset Home_and_Kitchen is completed\n",
      "chunk 162 of dataset Home_and_Kitchen is completed\n",
      "chunk 163 of dataset Home_and_Kitchen is completed\n",
      "chunk 164 of dataset Home_and_Kitchen is completed\n",
      "chunk 165 of dataset Home_and_Kitchen is completed\n",
      "chunk 166 of dataset Home_and_Kitchen is completed\n",
      "chunk 167 of dataset Home_and_Kitchen is completed\n",
      "chunk 168 of dataset Home_and_Kitchen is completed\n",
      "chunk 169 of dataset Home_and_Kitchen is completed\n",
      "chunk 170 of dataset Home_and_Kitchen is completed\n",
      "chunk 171 of dataset Home_and_Kitchen is completed\n",
      "chunk 172 of dataset Home_and_Kitchen is completed\n",
      "chunk 173 of dataset Home_and_Kitchen is completed\n",
      "chunk 174 of dataset Home_and_Kitchen is completed\n",
      "chunk 175 of dataset Home_and_Kitchen is completed\n",
      "chunk 176 of dataset Home_and_Kitchen is completed\n",
      "chunk 177 of dataset Home_and_Kitchen is completed\n",
      "chunk 178 of dataset Home_and_Kitchen is completed\n",
      "chunk 179 of dataset Home_and_Kitchen is completed\n",
      "chunk 180 of dataset Home_and_Kitchen is completed\n",
      "chunk 181 of dataset Home_and_Kitchen is completed\n",
      "chunk 182 of dataset Home_and_Kitchen is completed\n",
      "chunk 183 of dataset Home_and_Kitchen is completed\n",
      "chunk 184 of dataset Home_and_Kitchen is completed\n",
      "chunk 185 of dataset Home_and_Kitchen is completed\n",
      "chunk 186 of dataset Home_and_Kitchen is completed\n",
      "chunk 187 of dataset Home_and_Kitchen is completed\n",
      "chunk 188 of dataset Home_and_Kitchen is completed\n",
      "chunk 189 of dataset Home_and_Kitchen is completed\n",
      "chunk 190 of dataset Home_and_Kitchen is completed\n",
      "chunk 191 of dataset Home_and_Kitchen is completed\n",
      "chunk 192 of dataset Home_and_Kitchen is completed\n",
      "chunk 193 of dataset Home_and_Kitchen is completed\n",
      "chunk 194 of dataset Home_and_Kitchen is completed\n",
      "chunk 195 of dataset Home_and_Kitchen is completed\n",
      "chunk 196 of dataset Home_and_Kitchen is completed\n",
      "chunk 197 of dataset Home_and_Kitchen is completed\n",
      "chunk 198 of dataset Home_and_Kitchen is completed\n",
      "chunk 199 of dataset Home_and_Kitchen is completed\n",
      "chunk 200 of dataset Home_and_Kitchen is completed\n",
      "chunk 201 of dataset Home_and_Kitchen is completed\n",
      "chunk 202 of dataset Home_and_Kitchen is completed\n",
      "chunk 203 of dataset Home_and_Kitchen is completed\n",
      "chunk 204 of dataset Home_and_Kitchen is completed\n",
      "chunk 205 of dataset Home_and_Kitchen is completed\n",
      "chunk 206 of dataset Home_and_Kitchen is completed\n",
      "chunk 207 of dataset Home_and_Kitchen is completed\n",
      "chunk 208 of dataset Home_and_Kitchen is completed\n",
      "chunk 209 of dataset Home_and_Kitchen is completed\n",
      "chunk 210 of dataset Home_and_Kitchen is completed\n",
      "chunk 211 of dataset Home_and_Kitchen is completed\n",
      "chunk 212 of dataset Home_and_Kitchen is completed\n",
      "chunk 213 of dataset Home_and_Kitchen is completed\n",
      "chunk 214 of dataset Home_and_Kitchen is completed\n",
      "chunk 215 of dataset Home_and_Kitchen is completed\n",
      "chunk 216 of dataset Home_and_Kitchen is completed\n",
      "chunk 217 of dataset Home_and_Kitchen is completed\n",
      "chunk 218 of dataset Home_and_Kitchen is completed\n",
      "chunk 219 of dataset Home_and_Kitchen is completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished executing after 334.91457986831665 seconds\n",
      "start json_to_csv: Tools_and_Home_Improvement\n",
      "finished threading \"Tools_and_Home_Improvement\" after 0.00 seconds\n",
      "\n",
      "done converting \"json_files/Tools_and_Home_Improvement.json\" to \"csv_files/Tools_and_Home_Improvement.csv\", 9015203 lines \n",
      "run time: 76.926 seconds\n",
      "chunk 0 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 1 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 2 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 3 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 4 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 5 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 6 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 7 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 8 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 9 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 10 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 11 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 12 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 13 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 14 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 15 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 16 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 17 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 18 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 19 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 20 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 21 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 22 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 23 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 24 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 25 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 26 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 27 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 28 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 29 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 30 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 31 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 32 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 33 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 34 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 35 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 36 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 37 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 38 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 39 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 40 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 41 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 42 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 43 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 44 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 45 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 46 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 47 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 48 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 49 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 50 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 51 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 52 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 53 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 54 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 55 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 56 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 57 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 58 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 59 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 60 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 61 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 62 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 63 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 64 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 65 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 66 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 67 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 68 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 69 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 70 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 71 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 72 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 73 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 74 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 75 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 76 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 77 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 78 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 79 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 80 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 81 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 82 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 83 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 84 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 85 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 86 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 87 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 88 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 89 of dataset Tools_and_Home_Improvement is completed\n",
      "chunk 90 of dataset Tools_and_Home_Improvement is completed\n",
      "finished executing after 129.7226369380951 seconds\n",
      "finished executing \"Office_Products\" after 1504.32 seconds\n",
      "finished executing \"Arts_Crafts_and_Sewing\" after 1382.69 seconds\n",
      "finished executing \"Electronics\" after 1329.27 seconds\n",
      "finished executing \"Home_and_Kitchen\" after 726.23 seconds\n",
      "finished executing \"Tools_and_Home_Improvement\" after 206.66 seconds\n",
      "Run time of all datasets is: 1504.32 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # calculate time of running\n",
    "threads = [] # create a list of Concurrent Fututre objects\n",
    "datasets = ['Office_Products','Arts_Crafts_and_Sewing', 'Electronics', 'Home_and_Kitchen', 'Tools_and_Home_Improvement']\n",
    "phrases = ['im left handed', 'i am left handed']\n",
    "Reviews_list = []\n",
    "times = {} # dictionary of statring times\n",
    "for dataset in datasets:\n",
    "    times[dataset] = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        reviews_dataset = Reviews(name=dataset, path='json_files/'+dataset+'.json', columns=['reviewerID','asin','overall','reviewText'])\n",
    "#         reviews_dataset.execute(phrases=phrases) # execute all the data analysis functions \n",
    "        thrd = executor.submit(reviews_dataset.execute, phrases=phrases) # assign execute function to thrd \n",
    "        #     reviews_dataset.zeroize_df_reviews() # clear the RAM space\n",
    "        Reviews_list.append(reviews_dataset)\n",
    "        threads.append(thrd)\n",
    "        print(f'\\nfinished threading \\\"{dataset}\\\" after {time.time() - times[dataset]:.2f} seconds')\n",
    "times = list(times.values())\n",
    "for thrd in threads: # print if thread is done, backup solution were not asked.\n",
    "    if thrd.done():\n",
    "        print(f'finished executing \\\"{datasets[threads.index(thrd)]}\\\" after {time.time() - times[threads.index(thrd)]:.2f} seconds')\n",
    "print(f'Run time of all datasets is: {time.time() - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words distirubtion (plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for review_object in Reviews_list: # iterate over the list of Reviews datasets \n",
    "    review_object.disribute_left_reviews_length() # method that plot the number of words per reviews (counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the results (datasets of slied reviews) to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review_object in Reviews_list:\n",
    "    review_object.export_results(threshold = 0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Noga1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
